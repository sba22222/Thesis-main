{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3lJKe6TGC8R",
    "outputId": "5133b1d8-2cc1-401b-eab6-f53160eb2fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 24 13:12:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gsfH-EU7KPi",
    "outputId": "c87a15e8-e6e1-45ea-a0cc-a627b41ffd22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Thesis-main'...\n",
      "remote: Enumerating objects: 25765, done.\u001b[K\n",
      "remote: Counting objects: 100% (6564/6564), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5756/5756), done.\u001b[K\n",
      "remote: Total 25765 (delta 88), reused 6540 (delta 66), pack-reused 19201\u001b[K\n",
      "Receiving objects: 100% (25765/25765), 79.60 MiB | 15.76 MiB/s, done.\n",
      "Resolving deltas: 100% (423/423), done.\n",
      "Updating files: 100% (36302/36302), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the 'Thesis-main' repository from GitHub.\n",
    "!git clone https://github.com/sba22222/Thesis-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbO3hmaF7T2e",
    "outputId": "2e166090-1b85-46c4-a690-d7313a1937c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: datasets in c:\\users\\naomi\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\naomi\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\naomi\\anaconda3\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\naomi\\anaconda3\\lib\\site-packages (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\naomi\\anaconda3\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (3.1.41)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (1.39.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (4.25.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: note_seq in c:\\users\\naomi\\anaconda3\\lib\\site-packages (0.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (2.0.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (22.1.0)\n",
      "Requirement already satisfied: bokeh>=0.12.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (3.2.1)\n",
      "Requirement already satisfied: intervaltree>=2.1.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (3.1.0)\n",
      "Requirement already satisfied: IPython in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (8.15.0)\n",
      "Requirement already satisfied: librosa>=0.6.2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (0.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.18.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (2.0.3)\n",
      "Requirement already satisfied: pretty-midi>=0.2.6 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (0.2.10)\n",
      "Requirement already satisfied: protobuf>=4.21.2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (4.25.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (0.25.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from note_seq) (1.11.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from bokeh>=0.12.0->note_seq) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from bokeh>=0.12.0->note_seq) (1.0.5)\n",
      "Requirement already satisfied: packaging>=16.8 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from bokeh>=0.12.0->note_seq) (23.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from bokeh>=0.12.0->note_seq) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from bokeh>=0.12.0->note_seq) (6.0)\n",
      "Requirement already satisfied: tornado>=5.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from bokeh>=0.12.0->note_seq) (6.3.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from bokeh>=0.12.0->note_seq) (2022.9.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from intervaltree>=2.1.0->note_seq) (2.4.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (0.57.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (4.7.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (0.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from librosa>=0.6.2->note_seq) (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pandas>=0.18.1->note_seq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pandas>=0.18.1->note_seq) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pandas>=0.18.1->note_seq) (2023.3)\n",
      "Requirement already satisfied: mido>=1.1.16 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pretty-midi>=0.2.6->note_seq) (1.3.2)\n",
      "Requirement already satisfied: six in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pretty-midi>=0.2.6->note_seq) (1.16.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (5.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from IPython->note_seq) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from jedi>=0.16->IPython->note_seq) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from Jinja2>=2.9->bokeh>=0.12.0->note_seq) (2.1.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa>=0.6.2->note_seq) (0.40.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa>=0.6.2->note_seq) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa>=0.6.2->note_seq) (2.31.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->note_seq) (0.2.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa>=0.6.2->note_seq) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa>=0.6.2->note_seq) (1.15.1)\n",
      "Requirement already satisfied: executing in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from stack-data->IPython->note_seq) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from stack-data->IPython->note_seq) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from stack-data->IPython->note_seq) (0.2.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.6.2->note_seq) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "# Install required Python packages using pip.\n",
    "!pip install datasets transformers evaluate accelerate\n",
    "!pip install wandb\n",
    "!pip install note_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os\n",
    "import wandb\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KN2VoNGM7XUi",
    "outputId": "4595bd98-55c7-43b1-9c59-e3968ec95afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "fluidsynth is already the newest version (2.2.5-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Check if the code is running in Google Colab environment\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Inform the user about installing dependencies in Colab\n",
    "    print(\"Installing dependencies...\")\n",
    "\n",
    "    # Install fluidsynth and its development libraries using pip\n",
    "    !apt-get install fluidsynth\n",
    "    !apt-get install -qq libasound2-dev libjack-dev\n",
    "\n",
    "    # Install the pyfluidsynth library using pip\n",
    "    !pip install -qU pyfluidsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qjrncEFe7ZWL"
   },
   "outputs": [],
   "source": [
    "# Set the Protocol Buffers Python implementation to \"python\"\n",
    "# This line is used to resolve compatibility issues related to Protocol Buffers (protobuf)\n",
    "# It explicitly selects the pure Python implementation of the protobuf library\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "f_iFGg-U7btB",
    "outputId": "a2339325-8a79-46da-8a27-924462c8bc8f"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login into wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "99477b5614da46af8c85c2129aabbc1f",
      "b01e823243ad481789ec21daaaa8cfa4",
      "bd683b60144a4d3980ace220fc500fdb",
      "83daea79fd3d4dc99573562787030d3e",
      "4f57227fcb224cdf8bc10408ba03b520",
      "d955cc44cd2548e58e8d26376b6aad5f",
      "e4b4b10e7fc442eb98e7949d6a5c0140",
      "aa9216786541467eacd9760165c729c3",
      "460f9ee0f77646ae9ce1a31c2d530add",
      "2623c46d93324adcb8f8f560a7606ad0",
      "c787d4662b624a9a9c02339b367ee3c3",
      "1eb265b11318437284c976045dc75236",
      "ae44570360b04d3fa775ee9102e79456",
      "73b07a32b75846b690ff51bf7be4b5c0",
      "6c6212d6143f446d80d05dbf1ed74510",
      "fc1029f167d54d649190e2fb2cc8e5b1",
      "6d75955e194b48b09aa2ec60336a62a0",
      "66e977f652b34237900fc23648d0205f",
      "e1515aef92cb452aa307686a47b1cc04",
      "4c0c7795359d4d3da0eb79d678130036",
      "694e4fe66a2340b28c682cc674ba930e",
      "96a0241ea5da4729a730de2ea9eed429",
      "1b44f6381d3d4d549203494363f18ded",
      "37ce34889510478fac3e7957afe3f0d6",
      "eba7a76a36c74e58ade421de91e69c83",
      "6c5655a63c464929b739bac23707efea",
      "0dad4a2f53ca4c8a91b7281e298b494e",
      "b73b58a0eb7146669c45ebad33fd16a4",
      "22f6000adbe246bca152a105ecdfe971",
      "6f26eaacdddc40b4a0d16d38932c8e7b",
      "e5a808dc35e84c58bb1347af81452236",
      "3b0512bf9dd04ab9927d7150ae51703b"
     ]
    },
    "id": "b66O3eTo_Xl6",
    "outputId": "c41d4f42-663b-42ad-f2b3-8c457047fbe5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99477b5614da46af8c85c2129aabbc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log in to the Hugging Face Hub from this notebook\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYBi7VhS7d2y",
    "outputId": "96a3a211-4757-47eb-e2b6-c103a8d804cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main\n"
     ]
    }
   ],
   "source": [
    "# Change the current working directory to 'Thesis-main/'\n",
    "%cd Thesis-main/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN1-by9X71_q",
    "outputId": "72635176-1deb-4219-c4f8-248d0bf907e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1SOLID_Data Exploration.ipynb'\t\t  params.py\n",
      "'2SOLID_Divide songs into Chunks.ipynb'   path\n",
      " 3SOLID_musicaiz_MMMTokenizer.ipynb\t 'Pickle file.ipynb'\n",
      " 4SOLID_Clean_data.ipynb\t\t  POP909\n",
      " 5SOLID_pretokenization_v3.ipynb\t  POP909_v2\n",
      " 5v2SOLID_pretokenization_v4.ipynb\t  POP909_v3\n",
      " 6SOLID_Model.ipynb\t\t\t  pop-pickle\n",
      " 6v2_SOLID_Model.ipynb\t\t\t  pop-pickle.zip\n",
      " 7SOLID_Hyperparameterisation.ipynb\t 'pretokenization v2.ipynb'\n",
      " 8SOLID_Tutorial_MMM_LMD_5_Sweeps.ipynb  'Read Me.txt'\n",
      " Archive\t\t\t\t  References.xlsx\n",
      "'CA2_SBA22222_Proposal v2.docx'\t\t  sweep.yaml\n",
      " create_dataset_mmm.py\t\t\t 'Thesis Write Up v2.docx'\n",
      " customtrainer.py\t\t\t 'Thesis Write Up v3.docx'\n",
      " logging.ipynb\t\t\t\t  tokenizer\n",
      " midi_data\t\t\t\t  tokenizer.json\n",
      " midi_data_v2\t\t\t\t  trained_tokenizer.json\n",
      " mmm_tokenizer_lmd_clean_SOLIDV2.ipynb\t  train.py\n",
      " output\t\t\t\t\t  utils.py\n",
      " output.xml\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the current directory.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNUoloqNCgol",
    "outputId": "d58b176f-37ea-405f-84af-95d93cbb7365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL='end'\n"
     ]
    }
   ],
   "source": [
    "# Set the environment variable 'WANDB_LOG_MODEL' to 'end'.\n",
    "# Setting the environment variable 'WANDB_LOG_MODEL' to 'end' is done to configure the logging behavior\n",
    "# in the Weights & Biases (wandb) platform. This specific configuration indicates that the logging of\n",
    "# models should end, used to control the duration or scope of model logging during an experiment.\n",
    "# The environment variable is accessed using '%env' in Jupyter Notebooks.\n",
    "%env WANDB_LOG_MODEL='end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMur9GROJt0U",
    "outputId": "c72bc7ec-2262-44e5-e3ad-c0d673acf6b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'end'\n"
     ]
    }
   ],
   "source": [
    "# Print the value of the 'WANDB_LOG_MODEL' environment variable.\n",
    "!echo $WANDB_LOG_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38XiKM2NChLz",
    "outputId": "3b14e54e-b414-4d9c-ac13-d377e547b605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH='all'\n"
     ]
    }
   ],
   "source": [
    "# Set the environment variable 'WANDB_WATCH' to 'all'.\n",
    "%env WANDB_WATCH='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spnFh669721a",
    "outputId": "fe1ac46f-f676-4a40-b54a-30fd373098a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 18:42:33.350416: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 18:42:33.350464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 18:42:33.351773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 18:42:34.574010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "usage: train.py [-h] [--output_dir OUTPUT_DIR] [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                [--evaluation_strategy EVALUATION_STRATEGY] [--save_strategy SAVE_STRATEGY]\n",
      "                [--eval_steps EVAL_STEPS] [--logging_steps LOGGING_STEPS]\n",
      "                [--logging_first_step LOGGING_FIRST_STEP] [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                [--save_steps SAVE_STEPS] [--lr_scheduler_type LR_SCHEDULER_TYPE]\n",
      "                [--learning_rate LEARNING_RATE] [--warmup_ratio WARMUP_RATIO]\n",
      "                [--weight_decay WEIGHT_DECAY] [--seed SEED]\n",
      "                [--load_best_model_at_end LOAD_BEST_MODEL_AT_END] [--report_to REPORT_TO]\n",
      "                [--prediction_loss_only PREDICTION_LOSS_ONLY] [--push_to_hub PUSH_TO_HUB]\n",
      "                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "\n",
      "Process hyper-parameters\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        Output directory\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        num train epochs\n",
      "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
      "                        per device train batch size\n",
      "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
      "                        per device eval batch size\n",
      "  --evaluation_strategy EVALUATION_STRATEGY\n",
      "                        evaluation strategy\n",
      "  --save_strategy SAVE_STRATEGY\n",
      "                        save strategy\n",
      "  --eval_steps EVAL_STEPS\n",
      "                        eval steps\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        logging steps\n",
      "  --logging_first_step LOGGING_FIRST_STEP\n",
      "                        logging first step\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT\n",
      "                        save total limit\n",
      "  --save_steps SAVE_STEPS\n",
      "                        save steps\n",
      "  --lr_scheduler_type LR_SCHEDULER_TYPE\n",
      "                        lr scheduler type\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        learning rate\n",
      "  --warmup_ratio WARMUP_RATIO\n",
      "                        warmup ratio\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        weight decay\n",
      "  --seed SEED           seed\n",
      "  --load_best_model_at_end LOAD_BEST_MODEL_AT_END\n",
      "                        load best model at end\n",
      "  --report_to REPORT_TO\n",
      "                        report to\n",
      "  --prediction_loss_only PREDICTION_LOSS_ONLY\n",
      "                        prediction loss only\n",
      "  --push_to_hub PUSH_TO_HUB\n",
      "                        push model to HF hub\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        gradient accumulation steps to compensate batch size\n"
     ]
    }
   ],
   "source": [
    "# Display the help information for the 'train.py' Python script.\n",
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kgv-VBor-vvp"
   },
   "source": [
    "# Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_BbB-3a768L",
    "outputId": "78cacad6-899e-4347-cac7-b0b60e216775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: sweep.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep with ID: \u001b[33mho9owhgg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909-pretokenization/sweeps/ho9owhgg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent musicgen/pop909-pretokenization/ho9owhgg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Trigger a sweep using the configuration defined in 'sweep.yaml' using wandb.\n",
    "!wandb sweep sweep.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNSVZ96r8Gry",
    "outputId": "f9621142-08d2-43bd-df88-76a725e3a135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wandb agent [OPTIONS] SWEEP_ID\n",
      "\n",
      "  Run the W&B agent\n",
      "\n",
      "Options:\n",
      "  -p, --project TEXT  The project of the sweep.\n",
      "  -e, --entity TEXT   The entity scope for the project.\n",
      "  --count INTEGER     The max number of runs for this agent.\n",
      "  --help              Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "# Display help information for the 'wandb agent' command.\n",
    "!wandb agent --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7HycP1zF1f7",
    "outputId": "e521b630-b52a-4fd0-a86b-f6ce025bc930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent ðŸ•µï¸\n",
      "2024-01-24 18:56:15,381 - wandb.wandb_agent - INFO - Running runs: []\n",
      "2024-01-24 18:56:15,768 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 18:56:15,768 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 2\n",
      "\tlearning_rate: 0.0009289080892233562\n",
      "2024-01-24 18:56:15,769 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=2 --learning_rate=0.0009289080892233562\n",
      "2024-01-24 18:56:19.502890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 18:56:19.502947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 18:56:19.504242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 18:56:20.740203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 18:56:20,776 - wandb.wandb_agent - INFO - Running runs: ['l0q2s1e1']\n",
      "Downloading readme: 100% 376/376 [00:00<00:00, 3.00MB/s]\n",
      "Downloading data: 100% 76.8M/76.8M [00:45<00:00, 1.68MB/s]\n",
      "Downloading data: 100% 70.7M/70.7M [00:30<00:00, 2.29MB/s]\n",
      "Downloading data: 100% 68.9M/68.9M [00:41<00:00, 1.65MB/s]\n",
      "Downloading data: 100% 67.2M/67.2M [00:28<00:00, 2.35MB/s]\n",
      "Downloading data: 100% 69.6M/69.6M [00:31<00:00, 2.22MB/s]\n",
      "Downloading data: 100% 67.3M/67.3M [00:28<00:00, 2.38MB/s]\n",
      "Downloading data: 100% 69.6M/69.6M [00:49<00:00, 1.42MB/s]\n",
      "Generating train split: 100% 177567/177567 [00:12<00:00, 13711.00 examples/s]\n",
      "tokenizer_config.json: 100% 146/146 [00:00<00:00, 872kB/s]\n",
      "tokenizer.json: 100% 602k/602k [00:00<00:00, 54.0MB/s]\n",
      "special_tokens_map.json: 100% 27.0/27.0 [00:00<00:00, 126kB/s]\n",
      "Map: 100% 159810/159810 [02:04<00:00, 1286.22 examples/s]\n",
      "Map: 100% 17757/17757 [00:13<00:00, 1318.17 examples/s]\n",
      "config.json: 100% 665/665 [00:00<00:00, 3.57MB/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190324-l0q2s1e1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msunny-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/l0q2s1e1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/9988 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 447772 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0% 0/9988 [00:01<?, ?it/s]\n",
      "2024-01-24 19:03:29,961 - wandb.wandb_agent - INFO - Cleaning up finished run: l0q2s1e1\n",
      "2024-01-24 19:03:30,351 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:03:30,351 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 1\n",
      "\tlearning_rate: 0.0027192618289645877\n",
      "2024-01-24 19:03:30,353 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=1 --learning_rate=0.0027192618289645877\n",
      "2024-01-24 19:03:34.080447: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:03:34.080497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:03:34.081785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:03:35.333216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:03:35,362 - wandb.wandb_agent - INFO - Running runs: ['xazidc26']\n",
      "Map: 100% 17757/17757 [00:14<00:00, 1229.18 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190359-xazidc26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/xazidc26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/19977 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 464756 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mstellar-sweep-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/xazidc26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190359-xazidc26/logs\u001b[0m\n",
      "2024-01-24 19:04:11,998 - wandb.wandb_agent - INFO - Cleaning up finished run: xazidc26\n",
      "2024-01-24 19:04:12,374 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:04:12,374 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 4\n",
      "\tlearning_rate: 0.0007403690296232116\n",
      "2024-01-24 19:04:12,375 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=4 --learning_rate=0.0007403690296232116\n",
      "2024-01-24 19:04:16.082132: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:04:16.082185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:04:16.083456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:04:17.333848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:04:17,385 - wandb.wandb_agent - INFO - Running runs: ['zlgasi8g']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190427-zlgasi8g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/zlgasi8g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/4994 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 466596 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mcool-sweep-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/zlgasi8g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190427-zlgasi8g/logs\u001b[0m\n",
      "2024-01-24 19:04:38,477 - wandb.wandb_agent - INFO - Cleaning up finished run: zlgasi8g\n",
      "2024-01-24 19:04:38,869 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:04:38,869 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 4\n",
      "\tlearning_rate: 0.0015874696866744532\n",
      "2024-01-24 19:04:38,870 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=4 --learning_rate=0.0015874696866744532\n",
      "2024-01-24 19:04:42.580214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:04:42.580268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:04:42.581510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:04:43.841825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:04:43,877 - wandb.wandb_agent - INFO - Running runs: ['9j7m7e3h']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190454-9j7m7e3h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbright-sweep-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9j7m7e3h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/4994 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 467652 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mbright-sweep-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9j7m7e3h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190454-9j7m7e3h/logs\u001b[0m\n",
      "2024-01-24 19:05:04,798 - wandb.wandb_agent - INFO - Cleaning up finished run: 9j7m7e3h\n",
      "2024-01-24 19:05:05,205 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:05:05,205 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 2\n",
      "\tlearning_rate: 0.0024830196160634606\n",
      "2024-01-24 19:05:05,206 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=2 --learning_rate=0.0024830196160634606\n",
      "2024-01-24 19:05:08.885454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:05:08.885508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:05:08.886815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:05:10.164348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:05:10,216 - wandb.wandb_agent - INFO - Running runs: ['9eiz4uu1']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190519-9eiz4uu1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mkind-sweep-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9eiz4uu1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/9988 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 468693 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mkind-sweep-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9eiz4uu1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190519-9eiz4uu1/logs\u001b[0m\n",
      "2024-01-24 19:05:31,164 - wandb.wandb_agent - ERROR - Detected 5 failed runs in a row, shutting down.\n",
      "2024-01-24 19:05:31,165 - wandb.wandb_agent - INFO - To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
     ]
    }
   ],
   "source": [
    "# Start the Weights & Biases (wandb) agent to run experiments specified by the sweep ID 'musicgen/pop909_protokenization/6ce95s8n'.\n",
    "# The '--count 50' option indicates that 50 parallel runs of the agent will be executed.\n",
    "!wandb agent musicgen/pop909_protokenization/6ce95s8n --count 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-3cH3wp-zU1"
   },
   "source": [
    "# Final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvTv6TEaGAsj",
    "outputId": "d3b75a84-90e0-4d06-c3d1-b813de1228a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 19:15:05.107992: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:15:05.108047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:15:05.109302: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:15:06.340981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 116, in _service_connect\n",
      "    svc_iface._svc_connect(port=port)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/service/service_sock.py\", line 30, in _svc_connect\n",
      "    self._sock_client.connect(port=port)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 102, in connect\n",
      "    s.connect((\"localhost\", port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 246, in train\n",
      "    run = wandb.init(project=params.wandb_project, job_type=\"training\", config=config)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1195, in init\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1172, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 189, in setup\n",
      "    self._wl = wandb_setup.setup(settings=setup_settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 327, in setup\n",
      "    ret = _setup(settings=settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 320, in _setup\n",
      "    wl = _WandbSetup(settings=settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 303, in __init__\n",
      "    _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 114, in __init__\n",
      "    self._setup()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 250, in _setup\n",
      "    self._setup_manager()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 277, in _setup_manager\n",
      "    self._manager = wandb_manager._Manager(settings=self._settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 153, in __init__\n",
      "    wandb._sentry.reraise(e)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 154, in reraise\n",
      "    raise exc.with_traceback(sys.exc_info()[2])\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 151, in __init__\n",
      "    self._service_connect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 125, in _service_connect\n",
      "    raise ManagerConnectionRefusedError(message)\n",
      "wandb.sdk.wandb_manager.ManagerConnectionRefusedError: Connection to wandb service failed: [Errno 111] Connection refused. \n"
     ]
    }
   ],
   "source": [
    "# Run the 'train.py' Python script with specified training parameters.\n",
    "\n",
    "# Training Parameters:\n",
    "# - Learning Rate: 0.0006058454513356471\n",
    "# - Per-device Training Batch Size: 16\n",
    "# - Number of Training Epochs: 20\n",
    "# - Push to Hub: True (indicating to push the trained model to the Hugging Face Model Hub)\n",
    "# - Evaluation Steps: 315\n",
    "# - Logging Steps: 315\n",
    "# - Save Steps: 315\n",
    "# - Output Directory: \"pop909-epochs10\"\n",
    "# - Gradient Accumulation Steps: 2\n",
    "\n",
    "!python train.py --learning_rate=0.0006058454513356471 --per_device_train_batch_size=16 --num_train_epochs=20 --push_to_hub=True --eval_steps=315 --logging_steps=315 --save_steps=315 --output_dir=\"pop909-epochs10\" --gradient_accumulation_steps=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV8g0GkCAm6I"
   },
   "source": [
    "Reference: https://colab.research.google.com/drive/1q5pIPPmYXeWagj3yYEWS0O4RBOQI7HNP?usp=sharing#scrollTo=YvTv6TEaGAsj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0dad4a2f53ca4c8a91b7281e298b494e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b44f6381d3d4d549203494363f18ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22f6000adbe246bca152a105ecdfe971",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6f26eaacdddc40b4a0d16d38932c8e7b",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "1eb265b11318437284c976045dc75236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22f6000adbe246bca152a105ecdfe971": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2623c46d93324adcb8f8f560a7606ad0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37ce34889510478fac3e7957afe3f0d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5a808dc35e84c58bb1347af81452236",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3b0512bf9dd04ab9927d7150ae51703b",
      "value": "Login successful"
     }
    },
    "3b0512bf9dd04ab9927d7150ae51703b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "460f9ee0f77646ae9ce1a31c2d530add": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c0c7795359d4d3da0eb79d678130036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f57227fcb224cdf8bc10408ba03b520": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_73b07a32b75846b690ff51bf7be4b5c0",
      "style": "IPY_MODEL_6c6212d6143f446d80d05dbf1ed74510",
      "tooltip": ""
     }
    },
    "66e977f652b34237900fc23648d0205f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1515aef92cb452aa307686a47b1cc04",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4c0c7795359d4d3da0eb79d678130036",
      "value": "Connecting..."
     }
    },
    "694e4fe66a2340b28c682cc674ba930e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eba7a76a36c74e58ade421de91e69c83",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6c5655a63c464929b739bac23707efea",
      "value": "Token is valid (permission: write)."
     }
    },
    "6c5655a63c464929b739bac23707efea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c6212d6143f446d80d05dbf1ed74510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "6d75955e194b48b09aa2ec60336a62a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f26eaacdddc40b4a0d16d38932c8e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73b07a32b75846b690ff51bf7be4b5c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83daea79fd3d4dc99573562787030d3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_1eb265b11318437284c976045dc75236",
      "style": "IPY_MODEL_ae44570360b04d3fa775ee9102e79456",
      "value": true
     }
    },
    "96a0241ea5da4729a730de2ea9eed429": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dad4a2f53ca4c8a91b7281e298b494e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b73b58a0eb7146669c45ebad33fd16a4",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "99477b5614da46af8c85c2129aabbc1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_694e4fe66a2340b28c682cc674ba930e",
       "IPY_MODEL_96a0241ea5da4729a730de2ea9eed429",
       "IPY_MODEL_1b44f6381d3d4d549203494363f18ded",
       "IPY_MODEL_37ce34889510478fac3e7957afe3f0d6"
      ],
      "layout": "IPY_MODEL_e4b4b10e7fc442eb98e7949d6a5c0140"
     }
    },
    "aa9216786541467eacd9760165c729c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae44570360b04d3fa775ee9102e79456": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b01e823243ad481789ec21daaaa8cfa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa9216786541467eacd9760165c729c3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_460f9ee0f77646ae9ce1a31c2d530add",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "b73b58a0eb7146669c45ebad33fd16a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd683b60144a4d3980ace220fc500fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_2623c46d93324adcb8f8f560a7606ad0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c787d4662b624a9a9c02339b367ee3c3",
      "value": ""
     }
    },
    "c787d4662b624a9a9c02339b367ee3c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d955cc44cd2548e58e8d26376b6aad5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc1029f167d54d649190e2fb2cc8e5b1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6d75955e194b48b09aa2ec60336a62a0",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "e1515aef92cb452aa307686a47b1cc04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4b4b10e7fc442eb98e7949d6a5c0140": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "e5a808dc35e84c58bb1347af81452236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eba7a76a36c74e58ade421de91e69c83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc1029f167d54d649190e2fb2cc8e5b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
