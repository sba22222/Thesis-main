{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03faa761",
   "metadata": {},
   "source": [
    "reference: https://colab.research.google.com/drive/1876dq54hRsWGWdsrC6E4cYdxmacb5a4S?usp=sharing#scrollTo=KDKaWJI2aAxZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f79ae",
   "metadata": {},
   "source": [
    "**Summary** <br>\n",
    "* We use the tokenizers library to create a tokenizer and train it on a sample text. <br>\n",
    "* The trained tokenizer is saved to a file (tokenizer.json) and then loaded back.<br>\n",
    "* The sample text is tokenized using the loaded tokenizer.<br>\n",
    "* A DataFrame is created from the tokens.<br>\n",
    "* We use WandB to log the vocabulary DataFrame as an artifact.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4df25",
   "metadata": {},
   "source": [
    "\"As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That's where the pre-tokenization step comes in. As we saw in Chapter 2, a word-based tokenizer can simply split a raw text into words on whitespace and punctuation.\" HF course. <br>\n",
    "\n",
    "In our case, this step is really simple, we need our pretokenization to split our text in \"words\" since our dataset is already a series of tokens. So a Whitespace pre_tokenizer would work fine here. The model we will use is, again, \"WordLevel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51c8ded",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (4.21.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
      "     -------------------------------------- 254.1/254.1 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (63.4.1)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "     -------------------------------------- 190.6/190.6 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (4.9.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-win_amd64.whl (11 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\naomi\\appdata\\roaming\\python\\python39\\site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\naomi\\appdata\\roaming\\python\\python39\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\naomi\\appdata\\roaming\\python\\python39\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020de89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers, models, pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392307d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIECE_START TRACK_START INST=0 BAR_START TIME_DELTA=24 NOTE_ON=70 NOTE_ON=59 NOTE_ON=63 NOTE_ON=47 TIME_DELTA=3 NOTE_OFF=70 TIME_DELTA=1 NOTE_ON=78 TIME_DELTA=3 NOTE_OFF=78 TIME_DELTA=2 NOTE_ON=66 NOTE_ON=80 NOTE_ON=54 NOTE_OFF=59 TIME_DELTA=\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.models.WordLevel' object has no attribute 'add_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m new_tokenizer\u001b[38;5;241m.\u001b[39mtrain_from_iterator(get_training_corpus())\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Add [UNK] token to the vocabulary\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mnew_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_tokens\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[0;32m     39\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainers\u001b[38;5;241m.\u001b[39mWordLevelTrainer(special_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tokenizers.models.WordLevel' object has no attribute 'add_tokens'"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"C:/Users/naomi/Thesis/Thesis/Thesis-main/tokenized_output_v2/all_tokenized_outputs.txt\"\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    all_tokenized_outputs = file.readlines()\n",
    "\n",
    "# Calculate the index for the 10% split\n",
    "split_index = int(0.1 * len(all_tokenized_outputs))\n",
    "\n",
    "# Extract the 11th sample (index 10)\n",
    "sample_10 = all_tokenized_outputs[10]\n",
    "\n",
    "# Take the first 242 characters of the sample\n",
    "sample = sample_10[:242]\n",
    "\n",
    "# Output the content of the sample variable\n",
    "print(sample)\n",
    "\n",
    "# Initialize Tokenizer\n",
    "new_tokenizer = Tokenizer(models.WordLevel())\n",
    "\n",
    "# Add pretokenizer\n",
    "new_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "# Yield batches of 1,000 texts\n",
    "def get_training_corpus():\n",
    "    dataset = all_tokenized_outputs  # Use all_tokenized_outputs directly\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000]\n",
    "\n",
    "# Train the tokenizer on your sample text\n",
    "new_tokenizer.train_from_iterator(get_training_corpus())\n",
    "\n",
    "# Add [UNK] token to the vocabulary\n",
    "new_tokenizer.model.add_tokens([\"[UNK]\"])\n",
    "\n",
    "# Trainer\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer\n",
    "new_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acfca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained tokenizer\n",
    "new_tokenizer.save(\"trained_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "loaded_tokenizer = Tokenizer.from_file(\"trained_tokenizer.json\")\n",
    "loaded_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f20891",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "WordLevel error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the sample text using the loaded tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m encoded\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[1;31mException\u001b[0m: WordLevel error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "# Tokenize the sample text using the loaded tokenizer\n",
    "encoded = loaded_tokenizer.encode(sample)\n",
    "tokens = encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e44513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the vocabulary\n",
    "vocab_df = pd.DataFrame(\n",
    "    [{\"Token\": token, \"Index\": idx} for idx, token in enumerate(tokens)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d414705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B run\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6683578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with vocab\n",
    "vocab_table = wandb.Table(data=vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40baa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artifact for raw data\n",
    "processed_data_at = wandb.Artifact(name=\"processed_data\", type=\"processed_data\")\n",
    "processed_data_at.add(vocab_table, name=\"vocab_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the artifact\n",
    "wandb.log_artifact(processed_data_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a06a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b4254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
