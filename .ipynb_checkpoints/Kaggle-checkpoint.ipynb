{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-11T18:10:43.103828Z",
     "iopub.status.busy": "2023-08-11T18:10:43.103465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (8.3.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (4.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (1.24.3)\n",
      "Requirement already satisfied: webcolors>=1.5 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (1.13)\n",
      "Requirement already satisfied: jsonpickle in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (3.0.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (10.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (3.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from music21) (2.27.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from matplotlib->music21) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from matplotlib->music21) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from matplotlib->music21) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from matplotlib->music21) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from matplotlib->music21) (23.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from matplotlib->music21) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from matplotlib->music21) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from requests->music21) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from requests->music21) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from requests->music21) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paperspace\\anaconda3\\lib\\site-packages (from requests->music21) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/archive/chopin'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "#Installing dependencies\n",
    "!pip install music21\n",
    "!apt-get install -y lilypond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">LSTM'S ALBUM RELEASE</p>\n",
    "\n",
    "<img src=\"https://github.com/KarnikaKapoor/Files/blob/main/music1.gif?raw=true\">\n",
    "\n",
    "<p style=\"font-family:newtimeroman;font-size:120%;color:#97BACB;\">In one of my previous notebooks, I created an RNN that generates lyrics. For such projects, we feed the network a series of strings and the network predicts the next string in the series based on the information it is trained on. This time I decided to use the same principle on the music. \n",
    "Full disclosure I am not a musician. However, I know a tiny bit of music theory and play the ukulele for fun. Nonetheless, If you are a musical novice, don't shy away dive right into this notebook. </p> \n",
    "\n",
    "<a id='top'></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    " <p style=\"font-family:newtimeroman;color:#97BACB#97BACB;font-size:120%;text-align:center;border-radius:40px 40px;\">TABLE OF CONTENTS</p>   \n",
    "    \n",
    "* [1. IMPORTING LIBRARIES](#1)\n",
    "    \n",
    "* [2. LOADING DATA](#2)\n",
    "    \n",
    "* [3. DATA EXPLORATION](#3)  \n",
    "    \n",
    "* [4. DATA PREPREPROCESSING](#4)  \n",
    "    \n",
    "* [5. MODEL BUILDING](#5) \n",
    "      \n",
    "* [6. EVALUATING MODELS](#6)\n",
    "    \n",
    "* [7. CONCLUSION](#7)\n",
    "    \n",
    "* [8. END](#8)\n",
    "\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">IMPORTING LIBRARIES</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paperspace\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "import tensorflow \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import random\n",
    "import IPython\n",
    "from IPython.display import Image, Audio\n",
    "import music21\n",
    "from music21 import *\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">LOADING DATA</p>\n",
    "\n",
    "For this project, I will be using MIDI files of classical piano music. The dataset includes various artists. I will be working with Frédéric Chopin's compositions. \n",
    " \n",
    "* First of all, I make a list of all the songs in the Chopin folder parsed as music21 stream.\n",
    "\n",
    "* Then I will be creating a function to extract chords and notes out of the data creating a corpus.\n",
    "\n",
    "**Laoding and parsing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code loads all the composers not just chopin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2938261823.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [7]\u001b[1;36m\u001b[0m\n\u001b[1;33m    This code loads all the composers not just chopin\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Base directory where all composers' folders are located\n",
    "base_filepath = \"../input/classical-music-midi/\"\n",
    "\n",
    "# Initializing an empty list to hold all the MIDI files\n",
    "all_midis = []\n",
    "\n",
    "# Looping through all the composers' folders\n",
    "for composer in os.listdir(base_filepath):\n",
    "    # Constructing the full path to the composer's folder\n",
    "    composer_filepath = os.path.join(base_filepath, composer)\n",
    "\n",
    "    # Checking if it's a directory before proceeding\n",
    "    if os.path.isdir(composer_filepath):\n",
    "        # Looping through all the files in the composer's folder\n",
    "        for i in os.listdir(composer_filepath):\n",
    "            if i.endswith(\".mid\"):\n",
    "                tr = os.path.join(composer_filepath, i)\n",
    "                midi = converter.parse(tr)\n",
    "                all_midis.append(midi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1169796679040",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mid\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     tr \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m+\u001b[39mi\n\u001b[1;32m----> 8\u001b[0m     midi \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     all_midis\u001b[38;5;241m.\u001b[39mappend(midi)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\converter\\__init__.py:1297\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(value, forceSource, number, format, **keywords)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parseData(value, number\u001b[38;5;241m=\u001b[39mnumber, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mbytes\u001b[39m)\n\u001b[0;32m   1296\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m _osCanLoad(valueStr)):\n\u001b[1;32m-> 1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parseFile(valueStr, number\u001b[38;5;241m=\u001b[39mnumber, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m   1298\u001b[0m                      forceSource\u001b[38;5;241m=\u001b[39mforceSource, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mbytes\u001b[39m)\n\u001b[0;32m   1300\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m _osCanLoad(common\u001b[38;5;241m.\u001b[39mcleanpath(valueStr))):\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parseFile(common\u001b[38;5;241m.\u001b[39mcleanpath(valueStr), number\u001b[38;5;241m=\u001b[39mnumber, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m   1302\u001b[0m                      forceSource\u001b[38;5;241m=\u001b[39mforceSource, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\converter\\__init__.py:1151\u001b[0m, in \u001b[0;36mparseFile\u001b[1;34m(fp, number, format, forceSource, **keywords)\u001b[0m\n\u001b[0;32m   1149\u001b[0m v \u001b[38;5;241m=\u001b[39m Converter()\n\u001b[0;32m   1150\u001b[0m fp \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mcleanpath(fp, returnPathlib\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1151\u001b[0m v\u001b[38;5;241m.\u001b[39mparseFile(fp, number\u001b[38;5;241m=\u001b[39mnumber, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, forceSource\u001b[38;5;241m=\u001b[39mforceSource, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mTYPE_CHECKING:\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v\u001b[38;5;241m.\u001b[39mstream, (stream\u001b[38;5;241m.\u001b[39mScore, stream\u001b[38;5;241m.\u001b[39mPart, stream\u001b[38;5;241m.\u001b[39mOpus))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\converter\\__init__.py:621\u001b[0m, in \u001b[0;36mConverter.parseFile\u001b[1;34m(self, fp, number, format, forceSource, storePickle, **keywords)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    620\u001b[0m     environLocal\u001b[38;5;241m.\u001b[39mprintDebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading original version\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparseFileNoPickle(fp, number, \u001b[38;5;28mformat\u001b[39m, forceSource, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m writePickle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fpPickle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m storePickle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;66;03m# save the stream to disk...\u001b[39;00m\n\u001b[0;32m    624\u001b[0m         environLocal\u001b[38;5;241m.\u001b[39mprintDebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreezing Pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\converter\\__init__.py:543\u001b[0m, in \u001b[0;36mConverter.parseFileNoPickle\u001b[1;34m(self, fp, number, format, forceSource, **keywords)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubConverter\u001b[38;5;241m.\u001b[39mkeywords \u001b[38;5;241m=\u001b[39m keywords\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubConverter\u001b[38;5;241m.\u001b[39mparseFile(\n\u001b[0;32m    544\u001b[0m         fp,\n\u001b[0;32m    545\u001b[0m         number\u001b[38;5;241m=\u001b[39mnumber,\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords\n\u001b[0;32m    547\u001b[0m     )\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConverterFileException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile is not in a correct format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\converter\\subConverters.py:1205\u001b[0m, in \u001b[0;36mConverterMidi.parseFile\u001b[1;34m(self, filePath, number, **keywords)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;124;03mGet MIDI data from a file path.\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03min defaults.quantizationQuarterLengthDivisors. (Default: (4, 3)).\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmusic21\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmidi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m translate \u001b[38;5;28;01mas\u001b[39;00m midiTranslate\n\u001b[1;32m-> 1205\u001b[0m midiTranslate\u001b[38;5;241m.\u001b[39mmidiFilePathToStream(filePath, inputM21\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\midi\\translate.py:2705\u001b[0m, in \u001b[0;36mmidiFilePathToStream\u001b[1;34m(filePath, inputM21, **keywords)\u001b[0m\n\u001b[0;32m   2703\u001b[0m mf\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   2704\u001b[0m mf\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 2705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m midiFileToStream(mf, inputM21\u001b[38;5;241m=\u001b[39minputM21, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\midi\\translate.py:2876\u001b[0m, in \u001b[0;36mmidiFileToStream\u001b[1;34m(mf, inputM21, quantizePost, **keywords)\u001b[0m\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions21\u001b[38;5;241m.\u001b[39mStreamException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno tracks are defined in this MIDI file.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2874\u001b[0m \u001b[38;5;66;03m# create a stream for each track\u001b[39;00m\n\u001b[0;32m   2875\u001b[0m \u001b[38;5;66;03m# may need to check if tracks actually have event data\u001b[39;00m\n\u001b[1;32m-> 2876\u001b[0m midiTracksToStreams(mf\u001b[38;5;241m.\u001b[39mtracks,\n\u001b[0;32m   2877\u001b[0m                     ticksPerQuarter\u001b[38;5;241m=\u001b[39mmf\u001b[38;5;241m.\u001b[39mticksPerQuarterNote,\n\u001b[0;32m   2878\u001b[0m                     quantizePost\u001b[38;5;241m=\u001b[39mquantizePost,\n\u001b[0;32m   2879\u001b[0m                     inputM21\u001b[38;5;241m=\u001b[39ms,\n\u001b[0;32m   2880\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n\u001b[0;32m   2881\u001b[0m \u001b[38;5;66;03m# s._setMidiTracks(mf.tracks, mf.ticksPerQuarterNote)\u001b[39;00m\n\u001b[0;32m   2883\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\midi\\translate.py:2613\u001b[0m, in \u001b[0;36mmidiTracksToStreams\u001b[1;34m(midiTracks, ticksPerQuarter, quantizePost, inputM21, **keywords)\u001b[0m\n\u001b[0;32m   2610\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2611\u001b[0m         streamPart \u001b[38;5;241m=\u001b[39m conductorPart\n\u001b[1;32m-> 2613\u001b[0m     midiTrackToStream(mt,\n\u001b[0;32m   2614\u001b[0m                       ticksPerQuarter\u001b[38;5;241m=\u001b[39mticksPerQuarter,\n\u001b[0;32m   2615\u001b[0m                       quantizePost\u001b[38;5;241m=\u001b[39mquantizePost,\n\u001b[0;32m   2616\u001b[0m                       inputM21\u001b[38;5;241m=\u001b[39mstreamPart,\n\u001b[0;32m   2617\u001b[0m                       conductorPart\u001b[38;5;241m=\u001b[39mconductorPart,\n\u001b[0;32m   2618\u001b[0m                       isFirst\u001b[38;5;241m=\u001b[39m(mt \u001b[38;5;129;01mis\u001b[39;00m firstTrackWithNotes),\n\u001b[0;32m   2619\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n\u001b[0;32m   2621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\midi\\translate.py:2091\u001b[0m, in \u001b[0;36mmidiTrackToStream\u001b[1;34m(mt, ticksPerQuarter, quantizePost, inputM21, conductorPart, isFirst, quarterLengthDivisors, **keywords)\u001b[0m\n\u001b[0;32m   2088\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mgetElementsByClass(stream\u001b[38;5;241m.\u001b[39mMeasure):\n\u001b[0;32m   2089\u001b[0m         \u001b[38;5;66;03m# Gaps will be filled by makeRests, below, which now recurses\u001b[39;00m\n\u001b[0;32m   2090\u001b[0m         m\u001b[38;5;241m.\u001b[39mmakeVoices(inPlace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fillGaps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 2091\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakeTies\u001b[49m\u001b[43m(\u001b[49m\u001b[43minPlace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;66;03m# always need to fill gaps, as rests are not found in any other way\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m s\u001b[38;5;241m.\u001b[39mmakeRests(inPlace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fillGaps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeRangeFromBarDuration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\stream\\base.py:6499\u001b[0m, in \u001b[0;36mStream.makeTies\u001b[1;34m(self, meterStream, inPlace, displayTiedAccidentals, classFilterList)\u001b[0m\n\u001b[0;32m   6487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmakeTies\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   6488\u001b[0m              meterStream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6489\u001b[0m              inPlace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   6490\u001b[0m              displayTiedAccidentals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   6491\u001b[0m              classFilterList\u001b[38;5;241m=\u001b[39m(note\u001b[38;5;241m.\u001b[39mGeneralNote,),\n\u001b[0;32m   6492\u001b[0m              ):\n\u001b[0;32m   6493\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   6494\u001b[0m \u001b[38;5;124;03m    Calls :py:func:`~music21.stream.makeNotation.makeTies`.\u001b[39;00m\n\u001b[0;32m   6495\u001b[0m \n\u001b[0;32m   6496\u001b[0m \u001b[38;5;124;03m    Changed in v.4., inPlace=False by default.\u001b[39;00m\n\u001b[0;32m   6497\u001b[0m \u001b[38;5;124;03m    Added in v.7, `classFilterList`.\u001b[39;00m\n\u001b[0;32m   6498\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m-> 6499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmakeNotation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakeTies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeterStream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeterStream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minPlace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minPlace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayTiedAccidentals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayTiedAccidentals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassFilterList\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassFilterList\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\stream\\makeNotation.py:1312\u001b[0m, in \u001b[0;36mmakeTies\u001b[1;34m(s, meterStream, inPlace, displayTiedAccidentals, classFilterList)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     dst \u001b[38;5;241m=\u001b[39m mNext\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;66;03m# mNext.coreSelfActiveSite(eRemain)\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;66;03m# manually set activeSite\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# cannot use coreInsert here\u001b[39;00m\n\u001b[1;32m-> 1312\u001b[0m \u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meRemain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;66;03m# we are not sure that this element fits\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;66;03m# completely in the next measure, thus, need to\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;66;03m# continue processing each measure\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mNextAdd:\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;66;03m# environLocal.printDebug([\u001b[39;00m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;66;03m#    'makeTies() inserting mNext into returnObj',\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;66;03m#    mNext])\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\stream\\base.py:2257\u001b[0m, in \u001b[0;36mStream.insert\u001b[1;34m(self, offsetOrItemOrList, itemOrNone, ignoreSort, setActiveSite)\u001b[0m\n\u001b[0;32m   2254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoreGuardBeforeAddElement(element)\n\u001b[0;32m   2255\u001b[0m \u001b[38;5;66;03m# main insert procedure here\u001b[39;00m\n\u001b[1;32m-> 2257\u001b[0m storeSorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoreInsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2258\u001b[0m \u001b[43m                              \u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2259\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mignoreSort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignoreSort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msetActiveSite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetActiveSite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2261\u001b[0m updateIsFlat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m element\u001b[38;5;241m.\u001b[39misStream:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\stream\\core.py:111\u001b[0m, in \u001b[0;36mStreamCore.coreInsert\u001b[1;34m(self, offset, element, ignoreSort, setActiveSite)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     highestSortTuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elements[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msortTuple()\n\u001b[1;32m--> 111\u001b[0m     thisSortTuple \u001b[38;5;241m=\u001b[39m \u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msortTuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodify(offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m highestSortTuple \u001b[38;5;241m<\u001b[39m thisSortTuple:\n\u001b[0;32m    114\u001b[0m         storeSorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\music21\\base.py:2672\u001b[0m, in \u001b[0;36mMusic21Object.sortTuple\u001b[1;34m(self, useSite, raiseExceptionOnMiss)\u001b[0m\n\u001b[0;32m   2670\u001b[0m     insertIndex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msites\u001b[38;5;241m.\u001b[39msiteDict[\u001b[38;5;28mid\u001b[39m(useSite)]\u001b[38;5;241m.\u001b[39mglobalSiteIndex\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiveSite \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2672\u001b[0m     insertIndex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msites\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msiteDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactiveSite\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mglobalSiteIndex\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# for None, use this instead of default of -2.\u001b[39;00m\n\u001b[0;32m   2674\u001b[0m     insertIndex \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1169796679040"
     ]
    }
   ],
   "source": [
    "#Loading the list of chopin's midi files as stream \n",
    "filepath = 'C:\\\\Users\\\\paperspace\\\\Documents\\\\Thesis\\\\archive\\\\chopin\\\\'\n",
    "#Getting midi files\n",
    "all_midis= []\n",
    "for i in os.listdir(filepath):\n",
    "    if i.endswith(\".mid\"):\n",
    "        tr = filepath+i\n",
    "        midi = converter.parse(tr)\n",
    "        all_midis.append(midi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I get the components out of these streams of MIDI files. The midi files only have the piano included as mentioned in the dataset. So the components of the file would be either piano chords or piano notes. \n",
    "\n",
    "**Note:** The musical notes are the building blocks of the music. It pertains to a pitch associated with a specific audio vibration. Western music utilizes twelve musical notes. \n",
    "\n",
    "**Chord:** A group of notes that sound good together is a chord.\n",
    "\n",
    "The music21 stream that was created in the above cell contains both, chords and notes, we will extract them in the form of notes and obtain a series of notes in the musical composition.\n",
    "\n",
    "**The function to get the notes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helping function        \n",
    "def extract_notes(file):\n",
    "    notes = []\n",
    "    pick = None\n",
    "    for j in file:\n",
    "        songs = instrument.partitionByInstrument(j)\n",
    "        for part in songs.parts:\n",
    "            pick = part.recurse()\n",
    "            for element in pick:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return notes\n",
    "#Getting the list of notes as Corpus\n",
    "Corpus= extract_notes(all_midis)\n",
    "print(\"Total notes in all the Chopin midis in the dataset:\", len(Corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our data in the form of a corpus. A list of strings, if you will. Each string indicates a musical note. Let us explore this data corpus.\n",
    "\n",
    "<a id=\"3\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">DATA EXPLORATION</p>\n",
    "\n",
    "**In this section, I will be:**\n",
    "* Exploring the data Corpus\n",
    "* Examine all the notes in the Corpus \n",
    "* Simplifying our Corpus to Built a working model\n",
    "\n",
    "**Let us have a look at the first 50 values in our corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First fifty values in the Corpus:\", Corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these values indicate the notes, as mentioned above.\n",
    "\n",
    "**Printing the music sheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Lets write some functions that we need to look into the data\n",
    "def show(music):\n",
    "    display(Image(str(music.write(\"lily.png\"))))\n",
    "    \n",
    "def chords_n_notes(Snippet):\n",
    "    Melody = []\n",
    "    offset = 0 #Incremental\n",
    "    for i in Snippet:\n",
    "        #If it is chord\n",
    "        if (\".\" in i or i.isdigit()):\n",
    "            chord_notes = i.split(\".\") #Seperating the notes in chord\n",
    "            notes = [] \n",
    "            for j in chord_notes:\n",
    "                inst_note=int(j)\n",
    "                note_snip = note.Note(inst_note)            \n",
    "                notes.append(note_snip)\n",
    "                chord_snip = chord.Chord(notes)\n",
    "                chord_snip.offset = offset\n",
    "                Melody.append(chord_snip)\n",
    "        # pattern is a note\n",
    "        else: \n",
    "            note_snip = note.Note(i)\n",
    "            note_snip.offset = offset\n",
    "            Melody.append(note_snip)\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    Melody_midi = stream.Stream(Melody)   \n",
    "    return Melody_midi\n",
    "\n",
    "Melody_Snippet = chords_n_notes(Corpus[:100])\n",
    "show(Melody_Snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing the above sheet music** \n",
    "\n",
    "*As I could not play a midi file on the Kaggle interface, I have created a \".wav\" filetype of the same outside of this code. I am using it to create an audio interface. Let us have a listen to the data corpus.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to play audio or corpus\n",
    "print(\"Sample Audio From Data\")\n",
    "IPython.display.Audio(\"../input/music-generated-lstm/Corpus_Snippet.wav\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine all the notes in the Corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a count dictionary\n",
    "count_num = Counter(Corpus)\n",
    "print(\"Total unique notes in the Corpus:\", len(count_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the notes dictionary\n",
    "Notes = list(count_num.keys())\n",
    "Recurrence = list(count_num.values())\n",
    "#Average recurrenc for a note in Corpus\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "print(\"Average recurrenc for a note in Corpus:\", Average(Recurrence))\n",
    "print(\"Most frequent note in Corpus appeared:\", max(Recurrence), \"times\")\n",
    "print(\"Least frequent note in Corpus appeared:\", min(Recurrence), \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there are some very rare notes in the melody; some so rare that it was played only once in the whole data. This would create a lot of problems. (I did run into most of them while writing this piece)\n",
    "To spare us the error reports, let us have a look at the frequency of the notes. \n",
    "And for simplicity, I shall be eliminating some of the least occurring notes. I am sure Chopin wouldn't mind me messing with his masterpiece for science or would he? Either way, I may never know!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of Notes\n",
    "plt.figure(figsize=(18,3),facecolor=\"#97BACB\")\n",
    "bins = np.arange(0,(max(Recurrence)), 50) \n",
    "plt.hist(Recurrence, bins=bins, color=\"#97BACB\")\n",
    "plt.axvline(x=100,color=\"#DBACC1\")\n",
    "plt.title(\"Frequency Distribution Of Notes In The Corpus\")\n",
    "plt.xlabel(\"Frequency Of Chords in Corpus\")\n",
    "plt.ylabel(\"Number Of Chords\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided, I will be taking out the notes that were played less than 100 times. I mean, if Chopin liked them he would have played it a lot more often. So I create a list of rare notes in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a list of rare chords\n",
    "rare_note = []\n",
    "for index, (key, value) in enumerate(count_num.items()):\n",
    "    if value < 100:\n",
    "        m =  key\n",
    "        rare_note.append(m)\n",
    "        \n",
    "print(\"Total number of notes that occur less than 100 times:\", len(rare_note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eleminating the rare notes\n",
    "for element in Corpus:\n",
    "    if element in rare_note:\n",
    "        Corpus.remove(element)\n",
    "\n",
    "print(\"Length of Corpus after elemination the rare notes:\", len(Corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! This is the cleaned data Corpus that I will be using for the music generation.  \n",
    "In the next section, I will be preprocessing this Corpus for the training model. \n",
    "\n",
    "The workflow for this project involves,\n",
    "\n",
    "<p style=\"background-color:#EBDDD0;font-family:newtimeroman;color:#444160;text-align:center;font-size:120%;\">Loading Data ➡️ Preprocessing ➡️ Building Mapping Dictionary ➡️ Building Model ➡️ Generating Music</p>\n",
    "\n",
    "As I have loaded and explored the data,  I will proceed further by pre-processing the text.  \n",
    "\n",
    "\n",
    "<a id=\"4\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">DATA PREPROCESSING</p>\n",
    "\n",
    "Notes are basically sound waves. In music, we have certain specific combinations of Frequency and Wavelength standardized as said notes. Our Corpus has the name of that note. As we parsed the data at the time of loading we took the help of the music21 library (by nice people at MIT); The library fetches Frequency, Wavelength, duration etc for the given notes. \n",
    "\n",
    "\n",
    "\n",
    "**In this section, I will be performing the following:**\n",
    "\n",
    "**Creating a dictionary:** Creating a dictionary to map the notes and their indices. We have the note's name as a string the Corpus. For the computer, these names are just a symbol. So we create a dictionary to map each unique note in our Corpus to a number. And vice versa to retrieve the values at the time of prediction. This will be used to encode and decode the information going in and getting out of the RNN. \n",
    "\n",
    "**Encoding and Splitting the corpus:** Encoding and splitting the corpus into smaller sequences of equal length: At this point, the Corpus contain notes. We will encode this corpus and create small sequences of equal lengths of features and the corresponding targets. Each feature and target will contain the mapped index in the dictionary of the unique characters they signify. \n",
    "\n",
    "**Assigning X and y:** The labels are then resized and normalized. Whereas the targets are one-hot encoded. Ready to be sent to the RNN for the training, but before that let us built the RNN model. \n",
    "\n",
    "**Splitting Train and Seed datasets** To create music we will need to send some input to the RNN. For that, we will set aside a part of the data as seeds. We could have trained it all but I am no musician to come up with an input seed value. \n",
    "\n",
    "**Creating a list of sorted unique characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the unique characters present in my corpus to bult a mapping dic. \n",
    "symb = sorted(list(set(Corpus)))\n",
    "\n",
    "L_corpus = len(Corpus) #length of corpus\n",
    "L_symb = len(symb) #length of total unique characters\n",
    "\n",
    "#Building dictionary to access the vocabulary from indices and vice versa\n",
    "mapping = dict((c, i) for i, c in enumerate(symb))\n",
    "reverse_mapping = dict((i, c) for i, c in enumerate(symb))\n",
    "\n",
    "print(\"Total number of characters:\", L_corpus)\n",
    "print(\"Number of unique characters:\", L_symb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding and Splitting the Corpus as Labels and Targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Corpus in equal length of strings and output target\n",
    "length = 40\n",
    "features = []\n",
    "targets = []\n",
    "for i in range(0, L_corpus - length, 1):\n",
    "    feature = Corpus[i:i + length]\n",
    "    target = Corpus[i + length]\n",
    "    features.append([mapping[j] for j in feature])\n",
    "    targets.append(mapping[target])\n",
    "    \n",
    "    \n",
    "L_datapoints = len(targets)\n",
    "print(\"Total number of sequences in the Corpus:\", L_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X and normalize\n",
    "X = (np.reshape(features, (L_datapoints, length, 1)))/ float(L_symb)\n",
    "# one hot encode the output variable\n",
    "y = tensorflow.keras.utils.to_categorical(targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting Train and Seed datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking out a subset of data to be used as seed\n",
    "X_train, X_seed, y_train, y_seed = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">MODEL BUILDING</p>\n",
    "\n",
    "I will be employing an LSTM for this project.\n",
    "\n",
    "**Following steps are involved in the model building**\n",
    "\n",
    "* Initialising the Model\n",
    "* Defining by adding layers\n",
    "* Compiling the Model\n",
    "* Training the Model\n",
    "\n",
    "**Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#Initialising the Model\n",
    "model = Sequential()\n",
    "#Adding layers\n",
    "model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "#Compiling the model for training  \n",
    "opt = Adamax(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model's Summary               \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#Training the Model\n",
    "history = model.fit(X_train, y_train, batch_size=256, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">EVALUATING MODELS</p>\n",
    "\n",
    "Now that I have my model trained on the MIDI files of piano music, let us see how it performs. \n",
    "\n",
    "**To evaluate my model, I shall be having a look at:**\n",
    "* The performance of the model via Learning Curves\n",
    "* The melody created\n",
    "\n",
    "**Plotting the learning curve for the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learnings \n",
    "history_df = pd.DataFrame(history.history)\n",
    "fig = plt.figure(figsize=(15,4), facecolor=\"#97BACB\")\n",
    "fig.suptitle(\"Learning Plot of Model for Loss\")\n",
    "pl=sns.lineplot(data=history_df[\"loss\"],color=\"#444160\")\n",
    "pl.set(ylabel =\"Training Loss\")\n",
    "pl.set(xlabel =\"Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating the Melody**\n",
    "\n",
    "A function to obtain the generated music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Malody_Generator(Note_Count):\n",
    "    seed = X_seed[np.random.randint(0,len(X_seed)-1)]\n",
    "    Music = \"\"\n",
    "    Notes_Generated=[]\n",
    "    for i in range(Note_Count):\n",
    "        seed = seed.reshape(1,length,1)\n",
    "        prediction = model.predict(seed, verbose=0)[0]\n",
    "        prediction = np.log(prediction) / 1.0 #diversity\n",
    "        exp_preds = np.exp(prediction)\n",
    "        prediction = exp_preds / np.sum(exp_preds)\n",
    "        index = np.argmax(prediction)\n",
    "        index_N = index/ float(L_symb)   \n",
    "        Notes_Generated.append(index)\n",
    "        Music = [reverse_mapping[char] for char in Notes_Generated]\n",
    "        seed = np.insert(seed[0],len(seed[0]),index_N)\n",
    "        seed = seed[1:]\n",
    "    #Now, we have music in form or a list of chords and notes and we want to be a midi file.\n",
    "    Melody = chords_n_notes(Music)\n",
    "    Melody_midi = stream.Stream(Melody)   \n",
    "    return Music,Melody_midi\n",
    "\n",
    "\n",
    "#getting the Notes and Melody created by the model\n",
    "Music_notes, Melody = Malody_Generator(100)\n",
    "show(Melody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sure looks like music! To check if it sounds like music we have to listen to the MIDI file. Playing midi is crumblesome. I have saved and converted a few generated melodies to \".wav\" format outside of this notebook. So let us have a listen. \n",
    "\n",
    "**Melody Generated Sample 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the generated melody\n",
    "Melody.write('midi','Melody_Generated.mid')\n",
    "#to play audio or corpus\n",
    "IPython.display.Audio(\"../input/music-generated-lstm/Melody_Generated 2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melody Generated Sample 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to play audio or corpus\n",
    "IPython.display.Audio(\"../input/music-generated-lstm/Melody_Generated_1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">CONCLUSION</p>\n",
    "<p style=\"font-family:newtimeroman;font-size:120%;color:#97BACB\">On inspecting the generated melody, I am quite satisfied. Unlike the lyrics project, The music doesn't have to hold true to the grammatical syntax. On the question, is it a good musical composition; is it artsy? Did our LSTM create a masterpiece? I don't know! I am not a connoisseur of music. I used a basic RNN and it worked alright.</p> \n",
    "\n",
    "<img src=\"https://github.com/KarnikaKapoor/Files/blob/main/music2.gif?raw=true\">     \n",
    "     \n",
    "     \n",
    "<p style=\"font-family:newtimeroman;font-size:120%;color:#97BACB\">So I decided to let the model have the fame it deserved. I am releasing the album! It is out on my blog. Don't forget to get your copy!</p>\n",
    "\n",
    "**Album** [Down The Uncanny Valley: Album Release](https://karnikakapoor.blogspot.com/2021/10/down-uncanny-valley.html)\n",
    "\n",
    "**Some Useful Resources:**\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "[MuseNet](https://openai.com/blog/musenet/) \n",
    "\n",
    "[Lyrics Generator](https://www.kaggle.com/karnikakapoor/lyrics-generator-rnn)\n",
    "\n",
    "\n",
    "**<span style=\"color:#DBACC1;\"> If you liked this Notebook, please do upvote.</span>**\n",
    "\n",
    "**<span style=\"color:#DBACC1;\"> If you have any questions, feel free to comment!</span>**\n",
    "\n",
    "**<span style=\"color:#DBACC1;\"> Best Wishes!</span>**\n",
    "\n",
    "<a id=\"8\"></a>\n",
    "# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">END</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
