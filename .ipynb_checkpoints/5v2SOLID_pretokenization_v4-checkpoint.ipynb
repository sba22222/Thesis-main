{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this code you need to first create a hugging face account. Please see how to do this [here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPmkrdB1s18k"
   },
   "source": [
    "## Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yPGNyAscsvzD"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets transformers\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_dataset function from the 'datasets' library for dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import AutoTokenizer from the 'transformers' library for handling tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import Tokenizer, WordLevel model, WhitespaceSplit pre_tokenizer, and WordLevelTrainer from the 'tokenizers' library\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# Import PreTrainedTokenizerFast from the 'transformers' library for efficient tokenization\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Import notebook_login from the 'huggingface_hub' library for logging into the Hugging Face Model Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Import pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Import wandb for logging experiments with Weights & Biases\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for WandB (Weights & Biases) integration\n",
    "wandb_project = \"pop909_musicgen\"\n",
    "entity = \"musicgen\"\n",
    "data_processed = \"pop909_processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4vQF0ZdtPzP"
   },
   "source": [
    "## Download Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3Se37G7o30ap"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/naomi/.cache/huggingface/datasets/aimusicgen___parquet/aimusicgen--pop909_clean_data-8139a41134aae9f9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 29930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3326\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary library or module for loading datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset named \"aimusicgen/pop909_clean_data\" with the \"train\" split\n",
    "ds = load_dataset(\"aimusicgen/pop909_clean_data\", split=\"train\")\n",
    "\n",
    "# Split the loaded dataset into training and testing sets\n",
    "# Set test_size to 0.1, indicating that 10% of the data will be used for testing\n",
    "# Shuffle the data to ensure randomness in the selection of training and testing samples\n",
    "raw_datasets = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "# Display the resulting raw datasets, which now consist of training and testing sets\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea__1shIWfLO"
   },
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xll9nVvzWqYQ"
   },
   "source": [
    "Let's start by seeing how the default GPT-2 tokenizer works on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3M_cWJdcW41U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PIECE_START TRACK_START INST=0 BAR_START TIME_DELTA=24 NOTE_ON=58 NOTE_ON=70 NOTE_ON=74 TIME_DELTA=3 NOTE_OFF=70 NOTE_OFF=74 TIME_DELTA=6 NOTE_ON=70 NOTE_ON=74 NOTE_ON=58 NOTE_ON=62 NOTE_ON=67 TIME_DELTA=4 NOTE_OFF=70 NOTE_OFF=74 TIME_DELTA=5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the \"text\" field from the \"train\" split of the raw_datasets\n",
    "\n",
    "# Select the 11th sample (index 10 since indexing starts at 0) from the \"train\" split\n",
    "sample_10 = raw_datasets[\"train\"][\"text\"][10]\n",
    "\n",
    "# Extract a substring from the selected sample, taking the first 242 characters\n",
    "sample = sample_10[:242]\n",
    "\n",
    "# Display the resulting substring (sample)\n",
    "sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KI7Nv3ldWnpo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PI', 'EC', 'E', '_', 'ST', 'ART', 'ĠTR', 'ACK', '_', 'ST', 'ART', 'ĠINST', '=', '0', 'ĠBAR', '_', 'ST', 'ART', 'ĠTIME', '_', 'D', 'EL', 'TA', '=', '24', 'ĠNOTE', '_', 'ON', '=', '58', 'ĠNOTE', '_', 'ON', '=', '70', 'ĠNOTE', '_', 'ON', '=', '74', 'ĠTIME', '_', 'D', 'EL', 'TA', '=', '3', 'ĠNOTE', '_', 'OFF', '=', '70', 'ĠNOTE', '_', 'OFF', '=', '74', 'ĠTIME', '_', 'D', 'EL', 'TA', '=', '6', 'ĠNOTE', '_', 'ON', '=', '70', 'ĠNOTE', '_', 'ON', '=', '74', 'ĠNOTE', '_', 'ON', '=', '58', 'ĠNOTE', '_', 'ON', '=', '62', 'ĠNOTE', '_', 'ON', '=', '67', 'ĠTIME', '_', 'D', 'EL', 'TA', '=', '4', 'ĠNOTE', '_', 'OFF', '=', '70', 'ĠNOTE', '_', 'OFF', '=', '74', 'ĠTIME', '_', 'D', 'EL', 'TA', '=', '5']\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Print the tokens obtained by tokenizing the 'sample' using the GPT-2 tokenizer\n",
    "# The 'sample' is a text string, and the .tokens() method returns the list of tokens\n",
    "print(tokenizer(sample).tokens())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_mFOkvYXj00"
   },
   "source": [
    "Not the best since it's not familiar with this English vocabulary, the tokenizer is using quite a few subwords. The tokenizer process involves four steps: \n",
    "\n",
    "1. Normalization, \n",
    "2. Pretokenization, \n",
    "3. Applying the tokenizer model, and \n",
    "4. Postprocessing. \n",
    "\n",
    "Let's break down each step in our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3BDqfunZmwi"
   },
   "source": [
    "### 1. Normalization\n",
    "\n",
    "In the normalization step, we perform *\"general cleanup, such as eliminating unnecessary whitespace, converting to lowercase, and/or removing accents...\"* as per the HF course.\n",
    "\n",
    "Since our vocabulary is already normalized, there's no requirement to eliminate whitespace, convert to lowercase, or perform any additional cleanup. This step can be bypassed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDKaWJI2aAxZ"
   },
   "source": [
    "### 2. Pretokenization\n",
    "\n",
    "*\"As outlined in the upcoming sections, training a tokenizer solely on raw text is not feasible. The initial step involves breaking down the texts into smaller units, such as words. This is where the pre-tokenization step becomes crucial. As demonstrated in Chapter 2, a word-based tokenizer can easily segment raw text into words based on whitespace and punctuation.\"* HF course.\n",
    "\n",
    "In our scenario, this step is straightforward; our pretokenization aims to segment our text into \"words\" since our dataset is already a sequence of tokens. Therefore, a Whitespace pre_tokenizer would be suitable here. Once again, the chosen model is \"WordLevel\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uXenQmkeXjbQ"
   },
   "outputs": [],
   "source": [
    "# Initialize a new tokenizer with the WordLevel model\n",
    "# The 'unk_token=\"[UNK]\"' parameter sets the token to be used for unknown or out-of-vocabulary words\n",
    "new_tokenizer = Tokenizer(model=WordLevel(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Hmzkr1Pia44T"
   },
   "outputs": [],
   "source": [
    "# Assign the WhitespaceSplit pre_tokenizer to the 'pre_tokenizer' attribute of the new_tokenizer\n",
    "new_tokenizer.pre_tokenizer = WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_WzVrCZjXc4T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PIECE_START', (0, 11)),\n",
       " ('TRACK_START', (12, 23)),\n",
       " ('INST=0', (24, 30)),\n",
       " ('BAR_START', (31, 40)),\n",
       " ('TIME_DELTA=24', (41, 54)),\n",
       " ('NOTE_ON=58', (55, 65)),\n",
       " ('NOTE_ON=70', (66, 76)),\n",
       " ('NOTE_ON=74', (77, 87)),\n",
       " ('TIME_DELTA=3', (88, 100)),\n",
       " ('NOTE_OFF=70', (101, 112)),\n",
       " ('NOTE_OFF=74', (113, 124)),\n",
       " ('TIME_DELTA=6', (125, 137)),\n",
       " ('NOTE_ON=70', (138, 148)),\n",
       " ('NOTE_ON=74', (149, 159)),\n",
       " ('NOTE_ON=58', (160, 170)),\n",
       " ('NOTE_ON=62', (171, 181)),\n",
       " ('NOTE_ON=67', (182, 192)),\n",
       " ('TIME_DELTA=4', (193, 205)),\n",
       " ('NOTE_OFF=70', (206, 217)),\n",
       " ('NOTE_OFF=74', (218, 229)),\n",
       " ('TIME_DELTA=5', (230, 242))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test our pre_tokenizer\n",
    "new_tokenizer.pre_tokenizer.pre_tokenize_str(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA6hiwT1ch7J"
   },
   "source": [
    "### 3. Tokenizer model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SkY5sbQqb-WQ"
   },
   "outputs": [],
   "source": [
    "# This function will yield the samples to train our tokenizer\n",
    "# Define a function named 'get_training_corpus' to generate training data in chunks\n",
    "def get_training_corpus():\n",
    "  # Access the \"train\" split of the raw_datasets\n",
    "  dataset = raw_datasets[\"train\"]\n",
    "  \n",
    "  # Iterate through the dataset in chunks of 1000 samples\n",
    "  for i in range(0, len(dataset), 1000):\n",
    "    # Yield the \"text\" field of each chunk, creating a generator for training corpus\n",
    "    yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cVSePJPheV-D"
   },
   "outputs": [],
   "source": [
    "# Initialize a WordLevelTrainer for training a WordLevel tokenizer\n",
    "# The trainer is configured with special tokens including \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", and \"[MASK]\"\n",
    "trainer = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[UNK] (Unknown Token):**\n",
    "Represents unknown or out-of-vocabulary words. When the tokenizer encounters a word not present in its vocabulary, it replaces the unknown word with \"[UNK]\" during tokenization.Handles words that are not part of the model's training vocabulary, ensuring that even unseen words can be represented.<br>\n",
    "**[CLS] (Classification Token):**\n",
    "Represents the beginning of a sequence or a classification task. It is often used in conjunction with sequence classification tasks or sentence-level embeddings. Indicates the start of a sequence, helping models understand the structure of input data and facilitating tasks like sentence classification.<br>\n",
    "**[SEP] (Separator Token)**\n",
    "Represents the separation between two segments in a sequence. Commonly used to separate sentences or segments in tasks like question-answering or text generation. Helps the model distinguish between different parts of the input sequence, guiding the model to understand relationships between segments.<br>\n",
    "**[PAD] (Padding Token)**\n",
    "Represents padding in sequences. It is used to make sequences of variable lengths equal in size by adding padding tokens to shorter sequences. Ensures that input sequences have consistent lengths, allowing for efficient batch processing during training and inference.<br>\n",
    "**[MASK] (Mask Token)**\n",
    "Used in masked language modeling tasks. During training, some words are replaced with \"[MASK]\" tokens, and the model is tasked with predicting the original words based on the context. Supports the pre-training of language models by training the model to predict missing or masked words, enhancing its ability to understand context and relationships between words.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ACDL4HrOeqxR"
   },
   "outputs": [],
   "source": [
    "# Train the 'new_tokenizer' using the training data generated by the 'get_training_corpus' function\n",
    "# The training is performed with the specified 'trainer', which is a WordLevelTrainer\n",
    "new_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDfKKcxRfEs5"
   },
   "source": [
    "### 4. Post processing and save it to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1JnIQDVpfB8i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained 'new_tokenizer' to a file named \"tokenizer.json\"\n",
    "new_tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# Load the saved tokenizer from \"tokenizer.json\" and create a new instance of PreTrainedTokenizerFast\n",
    "new_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n",
    "\n",
    "# Add a special token, '[PAD]', to the loaded tokenizer\n",
    "new_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# We will receive '0' as output if saving was successful with no errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must create an 'Access Token' in hugging face with the same name as the new tokenizer to run the below. i.e. pop909_tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Z_87egVqfdj4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b816e4546874b0a87039c318e949f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log in to the Hugging Face Model Hub \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "aEWOdTATfhja"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aimusicgen/pop909_tokenizer/commit/6fe1ee932447b6cfe5e0f5cee3480e405d0136ea', commit_message='Upload tokenizer', commit_description='', oid='6fe1ee932447b6cfe5e0f5cee3480e405d0136ea', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the trained 'new_tokenizer' to the Hugging Face Model Hub \n",
    "new_tokenizer.push_to_hub(\"aimusicgen/pop909_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hl7Nvik4gC5A"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87d99f3053f488db2c31880f179d2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd88147e82f046d79cc61861ec60b255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/5.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a55556765443faadb50fb89eca6204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained tokenizer using the AutoTokenizer class from the Hugging Face Model Hub\n",
    "# The tokenizer is loaded from the repository named \"aimusicgen/pop909_tokenizer\"\n",
    "load_tokenizer = AutoTokenizer.from_pretrained(\"aimusicgen/pop909_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BHgHPrBggPG4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [100, 62, 60, 12, 9, 43, 49, 45, 10, 48, 44, 14, 49, 45, 43, 17, 19, 7, 48, 44, 8], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the 'sample' text using the pre-trained 'load_tokenizer'\n",
    "# The 'sample' text is expected to be processed into a sequence of tokens\n",
    "load_tokenizer(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see here is the tokenized text in the form of dictionaries.<br>\n",
    "\n",
    "**input_ids:** Represents the tokenized input sequence where each number corresponds to a specific token.Each number corresponds to a token in the vocabulary.<br>\n",
    "\n",
    "**token_type_ids:** Represents the segment or sentence to which each token belongs. All tokens in the sequence belong to the same segment or sentence (segment 0). <br>\n",
    "\n",
    "**attention_mask:** Represents the attention mask indicating which tokens should be attended to (have a value of 1) and which should be ignored (have a value of 0). All tokens in the sequence are attended to, indicating that none should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HCLmX2TMgQy5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PIECE_START',\n",
       " 'TRACK_START',\n",
       " 'INST=0',\n",
       " 'BAR_START',\n",
       " 'TIME_DELTA=24',\n",
       " 'NOTE_ON=58',\n",
       " 'NOTE_ON=70',\n",
       " 'NOTE_ON=74',\n",
       " 'TIME_DELTA=3',\n",
       " 'NOTE_OFF=70',\n",
       " 'NOTE_OFF=74',\n",
       " 'TIME_DELTA=6',\n",
       " 'NOTE_ON=70',\n",
       " 'NOTE_ON=74',\n",
       " 'NOTE_ON=58',\n",
       " 'NOTE_ON=62',\n",
       " 'NOTE_ON=67',\n",
       " 'TIME_DELTA=4',\n",
       " 'NOTE_OFF=70',\n",
       " 'NOTE_OFF=74',\n",
       " 'TIME_DELTA=5']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the 'sample' text using the pre-trained 'load_tokenizer'\n",
    "# Retrieve and display the list of tokens obtained from the tokenization process\n",
    "load_tokenizer(sample).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "QjRhfQAZgTRR"
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer using the AutoTokenizer class from the Hugging Face Model Hub\n",
    "# The tokenizer is loaded from the repository named \"aimusicgen/pop909_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aimusicgen/pop909_tokenizer\")\n",
    "\n",
    "# Tokenize the 'sample' text using the loaded tokenizer\n",
    "# Print and display the list of tokens obtained from the tokenization process\n",
    "print(tokenizer(sample).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZYzMkkHgMucc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BAR_END': 11,\n",
       " 'NOTE_OFF=55': 50,\n",
       " 'NOTE_ON=72': 47,\n",
       " 'NOTE_ON=60': 25,\n",
       " 'NOTE_ON=37': 124,\n",
       " 'NOTE_OFF=84': 103,\n",
       " 'NOTE_ON=92': 136,\n",
       " 'NOTE_ON=45': 91,\n",
       " 'NOTE_ON=51': 83,\n",
       " 'NOTE_ON=103': 176,\n",
       " 'NOTE_OFF=48': 80,\n",
       " 'NOTE_ON=102': 178,\n",
       " 'NOTE_ON=62': 17,\n",
       " 'NOTE_ON=30': 146,\n",
       " 'NOTE_OFF=69': 32,\n",
       " '[UNK]': 0,\n",
       " 'NOTE_ON=76': 53,\n",
       " 'NOTE_OFF=90': 129,\n",
       " 'NOTE_OFF=38': 115,\n",
       " 'TRACK_START': 62,\n",
       " 'NOTE_ON=73': 55,\n",
       " 'NOTE_ON=31': 150,\n",
       " 'NOTE_ON=32': 140,\n",
       " 'NOTE_ON=28': 154,\n",
       " 'NOTE_OFF=62': 16,\n",
       " 'NOTE_OFF=61': 36,\n",
       " 'PIECE_START': 100,\n",
       " 'NOTE_OFF=56': 58,\n",
       " 'NOTE_OFF=92': 135,\n",
       " 'NOTE_OFF=31': 149,\n",
       " 'NOTE_OFF=57': 34,\n",
       " 'NOTE_ON=83': 102,\n",
       " '[CLS]': 1,\n",
       " 'NOTE_ON=61': 37,\n",
       " 'NOTE_ON=90': 130,\n",
       " 'NOTE_ON=42': 108,\n",
       " 'NOTE_ON=105': 180,\n",
       " 'TIME_DELTA=3': 10,\n",
       " 'NOTE_OFF=34': 137,\n",
       " 'NOTE_OFF=102': 177,\n",
       " 'NOTE_ON=34': 138,\n",
       " 'NOTE_ON=43': 93,\n",
       " 'NOTE_ON=85': 114,\n",
       " 'NOTE_ON=96': 156,\n",
       " 'NOTE_OFF=77': 63,\n",
       " 'NOTE_OFF=63': 28,\n",
       " 'NOTE_ON=84': 104,\n",
       " 'NOTE_ON=33': 144,\n",
       " 'NOTE_ON=54': 66,\n",
       " 'NOTE_OFF=75': 56,\n",
       " 'NOTE_ON=56': 59,\n",
       " 'NOTE_ON=89': 126,\n",
       " 'NOTE_OFF=106': 183,\n",
       " 'NOTE_OFF=85': 113,\n",
       " 'NOTE_OFF=50': 76,\n",
       " 'NOTE_ON=41': 106,\n",
       " 'NOTE_OFF=39': 117,\n",
       " 'NOTE_ON=91': 132,\n",
       " 'NOTE_ON=93': 142,\n",
       " 'NOTE_ON=106': 184,\n",
       " 'NOTE_ON=58': 43,\n",
       " 'NOTE_OFF=73': 54,\n",
       " 'NOTE_ON=48': 81,\n",
       " 'NOTE_ON=53': 72,\n",
       " 'NOTE_ON=82': 95,\n",
       " 'NOTE_OFF=44': 98,\n",
       " 'NOTE_OFF=32': 139,\n",
       " 'NOTE_ON=66': 27,\n",
       " 'NOTE_OFF=58': 42,\n",
       " 'NOTE_ON=100': 168,\n",
       " 'NOTE_OFF=70': 48,\n",
       " 'NOTE_OFF=79': 69,\n",
       " 'NOTE_OFF=40': 109,\n",
       " 'NOTE_ON=59': 23,\n",
       " 'NOTE_ON=50': 77,\n",
       " 'TIME_DELTA=4': 7,\n",
       " 'NOTE_ON=26': 158,\n",
       " 'NOTE_ON=99': 172,\n",
       " 'NOTE_OFF=33': 143,\n",
       " 'TIME_DELTA=24': 9,\n",
       " 'NOTE_ON=70': 49,\n",
       " 'NOTE_OFF=45': 90,\n",
       " 'NOTE_OFF=53': 71,\n",
       " 'NOTE_OFF=86': 111,\n",
       " 'BAR_START': 12,\n",
       " 'NOTE_ON=88': 122,\n",
       " 'NOTE_OFF=101': 169,\n",
       " 'NOTE_OFF=65': 30,\n",
       " 'NOTE_ON=79': 70,\n",
       " 'TIME_DELTA=12': 15,\n",
       " 'NOTE_ON=98': 166,\n",
       " 'NOTE_ON=63': 29,\n",
       " 'NOTE_OFF=87': 119,\n",
       " 'NOTE_OFF=83': 101,\n",
       " 'NOTE_OFF=41': 105,\n",
       " 'NOTE_ON=46': 97,\n",
       " 'NOTE_OFF=43': 92,\n",
       " 'NOTE_OFF=37': 123,\n",
       " 'NOTE_ON=36': 128,\n",
       " 'NOTE_ON=24': 174,\n",
       " 'NOTE_OFF=72': 46,\n",
       " 'NOTE_OFF=46': 96,\n",
       " 'TIME_DELTA=2': 6,\n",
       " 'NOTE_OFF=25': 181,\n",
       " 'NOTE_ON=25': 182,\n",
       " 'NOTE_OFF=78': 73,\n",
       " 'NOTE_OFF=91': 131,\n",
       " 'NOTE_OFF=93': 141,\n",
       " 'NOTE_ON=75': 57,\n",
       " 'NOTE_ON=65': 31,\n",
       " 'NOTE_OFF=52': 67,\n",
       " 'NOTE_ON=49': 89,\n",
       " 'NOTE_OFF=42': 107,\n",
       " 'NOTE_OFF=49': 88,\n",
       " 'NOTE_OFF=97': 163,\n",
       " 'NOTE_ON=74': 45,\n",
       " 'NOTE_ON=47': 87,\n",
       " 'NOTE_OFF=68': 40,\n",
       " 'NOTE_OFF=64': 20,\n",
       " 'INST=0': 60,\n",
       " 'NOTE_OFF=51': 82,\n",
       " 'NOTE_ON=52': 68,\n",
       " 'NOTE_ON=81': 85,\n",
       " 'TIME_DELTA=8': 13,\n",
       " 'NOTE_OFF=88': 121,\n",
       " 'NOTE_OFF=98': 165,\n",
       " 'TIME_DELTA=6': 14,\n",
       " 'NOTE_ON=38': 116,\n",
       " 'NOTE_ON=35': 134,\n",
       " 'NOTE_ON=77': 64,\n",
       " 'NOTE_OFF=74': 44,\n",
       " 'NOTE_ON=71': 39,\n",
       " 'NOTE_OFF=28': 153,\n",
       " '[MASK]': 4,\n",
       " 'NOTE_OFF=54': 65,\n",
       " 'NOTE_ON=55': 51,\n",
       " 'NOTE_ON=78': 74,\n",
       " 'NOTE_ON=67': 19,\n",
       " 'NOTE_OFF=76': 52,\n",
       " 'NOTE_ON=57': 35,\n",
       " 'NOTE_OFF=89': 125,\n",
       " 'NOTE_OFF=60': 24,\n",
       " 'NOTE_OFF=96': 155,\n",
       " 'NOTE_OFF=29': 159,\n",
       " 'TIME_DELTA=5': 8,\n",
       " 'NOTE_ON=94': 148,\n",
       " 'NOTE_OFF=94': 147,\n",
       " 'NOTE_ON=97': 164,\n",
       " 'NOTE_OFF=95': 151,\n",
       " 'NOTE_OFF=99': 171,\n",
       " 'NOTE_OFF=103': 175,\n",
       " 'NOTE_OFF=82': 94,\n",
       " 'NOTE_OFF=105': 179,\n",
       " '[SEP]': 2,\n",
       " 'NOTE_OFF=59': 22,\n",
       " 'NOTE_OFF=81': 84,\n",
       " 'TIME_DELTA=16': 75,\n",
       " 'NOTE_ON=95': 152,\n",
       " 'NOTE_ON=27': 162,\n",
       " 'NOTE_OFF=80': 78,\n",
       " 'NOTE_ON=40': 110,\n",
       " 'NOTE_ON=29': 160,\n",
       " 'NOTE_OFF=27': 161,\n",
       " 'NOTE_ON=86': 112,\n",
       " 'NOTE_OFF=47': 86,\n",
       " 'NOTE_ON=87': 120,\n",
       " 'NOTE_ON=101': 170,\n",
       " 'NOTE_OFF=26': 157,\n",
       " '[PAD]': 3,\n",
       " 'TIME_DELTA=1': 5,\n",
       " 'NOTE_OFF=67': 18,\n",
       " 'NOTE_ON=69': 33,\n",
       " 'NOTE_OFF=71': 38,\n",
       " 'NOTE_OFF=35': 133,\n",
       " 'NOTE_OFF=30': 145,\n",
       " 'NOTE_ON=44': 99,\n",
       " 'NOTE_OFF=100': 167,\n",
       " 'NOTE_OFF=24': 173,\n",
       " 'NOTE_ON=68': 41,\n",
       " 'NOTE_ON=39': 118,\n",
       " 'NOTE_ON=64': 21,\n",
       " 'NOTE_OFF=66': 26,\n",
       " 'NOTE_ON=80': 79,\n",
       " 'NOTE_OFF=36': 127,\n",
       " 'TRACK_END': 61}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the vocabulary of the tokenizer using the 'get_vocab' method\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Display and store the obtained vocabulary\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary of the tokenizer is the equalivant as a metadata dictionary for the tokenizer we created. It maps tokens to numerical indices, providing information about the unique elements (tokens) present in the dataset.It outlines the structure and organization of the tokenization process, detailing how words or subwords are represented by numerical indices.\n",
    "\n",
    "Create a dataframe with the tokenizer's vocabulary to upload it to weights & bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Lg3UVX6oMxfL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[UNK]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>[MASK]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NOTE_ON=105</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NOTE_OFF=25</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>NOTE_ON=25</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NOTE_OFF=106</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NOTE_ON=106</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token  Index\n",
       "15          [UNK]      0\n",
       "32          [CLS]      1\n",
       "153         [SEP]      2\n",
       "168         [PAD]      3\n",
       "133        [MASK]      4\n",
       "..            ...    ...\n",
       "36    NOTE_ON=105    180\n",
       "103   NOTE_OFF=25    181\n",
       "104    NOTE_ON=25    182\n",
       "52   NOTE_OFF=106    183\n",
       "59    NOTE_ON=106    184\n",
       "\n",
       "[185 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame 'df' using a list comprehension, where each row contains a token and its corresponding index from\n",
    "# the tokenizer's vocabulary\n",
    "df = pd.DataFrame([{\"Token\": token, \"Index\": idx} for token, idx in vocab.items()]).sort_values(by=\"Index\")\n",
    "\n",
    "# Display the DataFrame 'df', which presents the tokens and their associated indices in the tokenizer's vocabulary\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9Z33hXEO94k"
   },
   "source": [
    "### Upload vocab to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "FeGFtbhMNMJl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\naomi\\Thesis\\Thesis\\Thesis-main\\wandb\\run-20240121_124240-ar11ktzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/musicgen/pop909-pretokenization/runs/ar11ktzp' target=\"_blank\">volcanic-shadow-3</a></strong> to <a href='https://wandb.ai/musicgen/pop909-pretokenization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/musicgen/pop909-pretokenization' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/musicgen/pop909-pretokenization/runs/ar11ktzp' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization/runs/ar11ktzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a Weights & Biases (wandb) run with project name is set to \"pop909_pretokenization\", \n",
    "# and the job type is set to \"upload\"\n",
    "run = wandb.init(project=wandb_project, job_type=\"upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0llSY2-jP8Rm"
   },
   "outputs": [],
   "source": [
    "# Create table with vocab\n",
    "vocab_table = wandb.Table(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If your framework uses or produces models or datasets, you can log them for full traceability and have wandb automatically monitor your entire pipeline through W&B Artifacts.\" - https://docs.wandb.ai/guides/integrations/add-wandb-to-any-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "wYc1ACQSQEFd"
   },
   "outputs": [],
   "source": [
    "# Create artifact for raw data\n",
    "processed_data_at = wandb.Artifact(name=data_processed, type=\"processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kNheYjlWQRF1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArtifactManifestEntry(path='vocab_table.table.json', digest='jGQiS9MBJFnW0TOd0jjl7w==', size=4226, local_path='C:\\\\Users\\\\naomi\\\\AppData\\\\Local\\\\wandb\\\\wandb\\\\artifacts\\\\staging\\\\tmp6lagf24e')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 'vocab_table' to 'processed_data_at' artifact with the name \"vocab_table\"\n",
    "processed_data_at.add(vocab_table, name=\"vocab_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "O-84ZVqDQ7iM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact pop909-processed>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log the 'processed_data_at' artifact to the Weights & Biases run\n",
    "run.log_artifact(processed_data_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "22YjV7gJRCXF"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.026 MB uploaded\\r'), FloatProgress(value=0.04438376115055525, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-shadow-3</strong> at: <a href='https://wandb.ai/musicgen/pop909-pretokenization/runs/ar11ktzp' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization/runs/ar11ktzp</a><br/> View job at <a href='https://wandb.ai/musicgen/pop909-pretokenization/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMjM0NDAzOA==/version_details/v0' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMjM0NDAzOA==/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240121_124240-ar11ktzp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complete and finish the Weights & Biases run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCxK_DEBREOs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
