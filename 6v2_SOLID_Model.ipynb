{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1. Why is the context length set to 2048? Look up hugging face documentation\n",
    "2. What is the number you put in for 'xquery = 20100000' what is it related to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yPGNyAscsvzD"
   },
   "outputs": [],
   "source": [
    "## Install necessary libraries\n",
    "# !pip install datasets\n",
    "# !pip install wandb\n",
    "# !pip install note_seq\n",
    "# !pip install transformers[torch]\n",
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naomi\\anaconda3\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import note_seq\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoConfig, GPT2LMHeadModel, DataCollatorForLanguageModeling, set_seed, Trainer, TrainingArguments\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "COgCtDY2bJpQ"
   },
   "outputs": [],
   "source": [
    "# Set the Protocol Buffers Python implementation to \"python\"\n",
    "# This line is used to resolve compatibility issues related to Protocol Buffers (protobuf)\n",
    "# It explicitly selects the pure Python implementation of the protobuf library\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for W&B \n",
    "wandb_project = \"pop909-pretokenization\"\n",
    "entity = \"musicgen\"\n",
    "data_processed = \"pop909-processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4vQF0ZdtPzP"
   },
   "source": [
    "## Download Dataset and tokenizer from Hugging Face\n",
    "\n",
    "In the pretokenization notebook, we trained a tokenizer. We'll use it here first to do some basic EDA to understand our data and what type of model size is better (number of layers, heads, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3Se37G7o30ap"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/naomi/.cache/huggingface/datasets/aimusicgen___parquet/aimusicgen--pop909_clean_data-8139a41134aae9f9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 29930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3326\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a dataset named \"aimusicgen/pop909_clean_data\" using the Hugging Face datasets library\n",
    "# The split parameter is set to \"train\" to load the training split of the dataset\n",
    "ds = load_dataset(\"aimusicgen/pop909_clean_data\", split=\"train\")\n",
    "\n",
    "# Split the loaded dataset into training and testing sets using train_test_split method\n",
    "# The test_size parameter specifies the fraction of the dataset to include in the test split (here, 10%)\n",
    "# The shuffle parameter is set to True to shuffle the data before splitting\n",
    "raw_datasets = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "# Instantiate a tokenizer using the AutoTokenizer class from the transformers library\n",
    "# The \"tokenizer\" argument is the pretrained tokenizer from the previous notebook\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aimusicgen/pop909_tokenizer\")\n",
    "\n",
    "# Display the raw datasets, which now include both training and testing splits\n",
    "# The raw_datasets variable now contains a tuple with two elements: training and testing datasets\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the data suffled before it is split into training and testing?\n",
    "\n",
    "The data is suffled to help introduce **randomness** into the data. This is important because it prevents any inherent order in the dataset from influencing the learning algorithm. If the data is ordered in a certain way (e.g., all samples of one class followed by another), shuffling helps ensure that the model sees a representative mix of samples from all classes during both training and testing.\n",
    "\n",
    "Some algorithms might perform differently or learn **biased** patterns if trained on data with a specific order. By shuffling the data, you reduce the risk of the model learning patterns based on the order of the examples.\n",
    "\n",
    "Shuffling contributes to better **generalization** of the model. If the model is exposed to diverse examples during training (rather than learning specific patterns related to the order of the data), it is more likely to perform well on new, unseen data.\n",
    "\n",
    "When splitting a dataset into training and testing sets, shuffling ensures that both sets contain a representative mix of examples. This is important, especially in scenarios like **cross-validation**, where you repeatedly split the data into different training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cCxK_DEBREOs"
   },
   "outputs": [],
   "source": [
    "# Define the context length for tokenization\n",
    "# In this case, it is set to 2048, meaning the input sequences will be truncated or padded to this length\n",
    "context_length = 2048\n",
    "\n",
    "# Define a tokenization function named \"tokenize\" that takes an element as input\n",
    "def tokenize(element):\n",
    "    # Use the tokenizer to process the \"text\" field of the input element\n",
    "    # Set truncation to True to truncate sequences longer than the specified context length\n",
    "    # Set max_length to the context_length to ensure all sequences have the same length\n",
    "    # Set padding to False to avoid adding padding tokens to the sequences\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    # Return a dictionary containing the \"input_ids\" field from the tokenizer outputs\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example from the training split of the raw dataset (index 1000)\n",
    "selected_example = raw_datasets[\"train\"][1000]\n",
    "\n",
    "# Tokenize the selected example using the tokenize function defined earlier\n",
    "# The tokenize function processes the \"text\" field of the input example\n",
    "# and returns a dictionary containing the \"input_ids\" field with tokenized representation\n",
    "tk_sample = tokenize(selected_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of tk_sample ids 413\n",
      "tk_sample {'input_ids': [74, 24, 22, 9, 5, 47, 10, 53, 61, 98, 11, 52, 60, 82, 46, 11, 49, 10, 19, 14, 18, 7, 30, 11, 29, 47, 6, 46, 11, 53, 10, 61, 52, 11, 60, 6, 59, 36, 47, 88, 7, 48, 81, 97, 13, 58, 6, 35, 46, 6, 59, 36, 47, 41, 58, 35, 7, 46, 87, 8, 9, 5, 59, 55, 38, 65, 41, 58, 54, 37, 7, 38, 55, 59, 12, 64, 6, 37, 14, 54, 14, 58, 14, 30, 47, 49, 53, 98, 5, 29, 46, 48, 52, 97, 8, 9, 5, 71, 65, 10, 78, 32, 70, 10, 71, 38, 6, 77, 11, 55, 55, 70, 11, 54, 54, 59, 59, 13, 58, 61, 6, 37, 7, 31, 64, 15, 60, 53, 65, 53, 47, 58, 5, 64, 52, 11, 46, 52, 8, 9, 5, 47, 47, 36, 7, 46, 7, 35, 46, 61, 53, 69, 11, 52, 60, 71, 19, 10, 70, 61, 36, 10, 60, 47, 61, 10, 46, 53, 12, 35, 7, 18, 52, 59, 6, 68, 12, 60, 58, 55, 59, 36, 94, 11, 58, 6, 69, 10, 40, 11, 35, 19, 13, 32, 6, 18, 7, 93, 68, 6, 31, 6, 39, 14, 54, 8, 23, 24, 22, 9, 5, 47, 10, 53, 61, 98, 11, 52, 60, 82, 46, 11, 49, 10, 19, 14, 18, 7, 30, 11, 29, 47, 6, 46, 11, 53, 10, 61, 52, 11, 60, 6, 59, 36, 47, 88, 7, 48, 81, 97, 13, 58, 6, 35, 46, 6, 59, 36, 47, 41, 58, 35, 7, 46, 87, 8, 9, 5, 59, 55, 38, 65, 41, 58, 54, 37, 7, 38, 55, 59, 12, 64, 6, 37, 14, 54, 14, 58, 14, 30, 47, 49, 53, 98, 5, 29, 46, 48, 52, 97, 8, 9, 5, 71, 65, 10, 78, 32, 70, 10, 71, 38, 6, 77, 11, 55, 55, 70, 11, 54, 54, 59, 59, 13, 58, 61, 6, 37, 7, 31, 64, 15, 60, 53, 65, 53, 47, 58, 5, 64, 52, 11, 46, 52, 8, 9, 5, 47, 47, 36, 7, 46, 7, 35, 46, 61, 53, 69, 11, 52, 60, 71, 19, 10, 70, 61, 36, 10, 60, 47, 61, 10, 46, 53, 12, 35, 7, 18, 52, 59, 6, 68, 12, 60, 58, 55, 59, 36, 94, 11, 58, 6, 69, 10, 40, 11, 35, 19, 13, 32, 6, 18, 7, 93, 68, 6, 31, 6, 39, 14, 54, 8, 23]}\n"
     ]
    }
   ],
   "source": [
    "# Print the length of the 'input_ids' field in the tokenized sample\n",
    "# This indicates the number of tokens in the tokenized representation of the input text\n",
    "print(f\"Len of tk_sample ids {len(tk_sample['input_ids'])}\")\n",
    "\n",
    "# Print the entire tokenized sample\n",
    "# The tokenized sample is a dictionary containing the 'input_ids' field\n",
    "print(f\"tk_sample {tk_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see only 742 tk_sample ids came back when there was an index of 1,000. This is possibly due to padding. It was set to *padding=False* in the *tokenize* function. Without padding, the resulting *input_ids* will not be padded to the maximum length, and if the original text is shorter than the specified *max_length*, the tokenized sequence will be shorter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4zunRo-CfEHr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 29930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3326\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the tokenize function to the entire raw dataset using the map method\n",
    "# The tokenize function is applied in a batched manner, improving efficiency\n",
    "# The remove_columns parameter is set to remove the columns from the raw dataset \n",
    "# (excluding the \"train\" split) after tokenization, as they are no longer needed\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize,                # The tokenization function to be applied\n",
    "    batched=True,            # Tokenize in batches for efficiency\n",
    "    remove_columns=raw_datasets[\"train\"].column_names  # Remove unnecessary columns after tokenization\n",
    ")\n",
    "\n",
    "# Display the resulting tokenized datasets\n",
    "# The tokenized_datasets variable now contains both training and testing splits with tokenized representations\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate optimal model size using Chinchilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Andrej Karpathy https://github.com/karpathy/nanoGPT/blob/master/scaling_laws.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Laws: Approach 2\n",
    "Approach 2 is probably my favorite one because it fixes a flop budget and runs a number of model/dataset sizes, measures the loss, fits a parabolla, and gets the minimum. So it's a fairly direct measurement of what we're after. The best way to then calculate the compute-optimal number of tokens for any given model size, as an example, is via simple interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1 numbers\n",
    "# # parameters, tokens\n",
    "# raw = [\n",
    "#     [400e6, 8e9],\n",
    "#     [1e9, 20.2e9],\n",
    "#     [10e9, 205.1e9],\n",
    "#     [67e9, 1.5e12],\n",
    "#     [175e9, 3.7e12],\n",
    "#     [280e9, 5.9e12],\n",
    "#     [520e9, 11e12],\n",
    "#     [1e12, 21.2e12],\n",
    "#     [10e12, 216.2e12],\n",
    "# ]\n",
    "\n",
    "# Approach 2 numbers\n",
    "# parameters, tokens\n",
    "raw = [\n",
    "    [400e6, 7.7e9],\n",
    "    [1e9, 20.0e9],\n",
    "    [10e9, 219.5e9],\n",
    "    [67e9, 1.7e12],\n",
    "    [175e9, 4.3e12],\n",
    "    [280e9, 7.1e12],\n",
    "    [520e9, 13.4e12],\n",
    "    [1e12, 26.5e12],\n",
    "    [10e12, 292.0e12],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1.0409573169995907x + 0.9353887152390612\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array 'x' containing the logarithm (base 10) of the first element of each sublist in 'raw'\n",
    "x = np.array([np.log10(x[0]) for x in raw])\n",
    "\n",
    "# Create a NumPy array 'y' containing the logarithm (base 10) of the second element of each sublist in 'raw'\n",
    "y = np.array([np.log10(x[1]) for x in raw])\n",
    "\n",
    "# Stack the arrays 'x' and a vector of ones to form a matrix 'A'\n",
    "# The resulting matrix 'A' is used for linear regression\n",
    "A = np.vstack([x, np.ones(len(x))]).T\n",
    "\n",
    "# Use NumPy's least squares solver to perform linear regression\n",
    "# The result is a tuple where the first element is the slope ('m') and the second element is the y-intercept ('c')\n",
    "m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "\n",
    "# Print the linear regression equation in the form y = mx + c\n",
    "print(f\"y = {m}x + {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new figure for the plot with a specified size\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "# Plot the linear regression line\n",
    "# For each point in 'raw', compute the corresponding y-value using the linear regression equation\n",
    "plt.plot([q[0] for q in raw], [10**(m*np.log10(q[0]) + c) for q in raw], label='linear regression', color='r')\n",
    "\n",
    "# Scatter plot of the raw data points\n",
    "# Each point is represented by its x and y values from the 'raw' list\n",
    "plt.scatter([q[0] for q in raw], [q[1] for q in raw], label='raw data')\n",
    "\n",
    "# Set the x-axis and y-axis to logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Set the labels for the x-axis and y-axis\n",
    "plt.xlabel('parameters')\n",
    "plt.ylabel('tokens')\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('compute optimal models')\n",
    "\n",
    "# Display a grid in the plot\n",
    "plt.grid()\n",
    "\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted parameters for 20,100,000.00 tokens: 344,908,651.96\n"
     ]
    }
   ],
   "source": [
    "# Define the query model size (e.g., GPT-2 small is 124M)\n",
    "xquery = 20100000\n",
    "\n",
    "# Use the linear regression equation to predict the parameters for the given model size\n",
    "yquery = 10**(m*np.log10(xquery) + c)\n",
    "\n",
    "# Print the predicted parameters for the specified model size\n",
    "# The model size is formatted with commas for better readability\n",
    "print(f\"predicted parameters for {xquery:,.2f} tokens: {yquery:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CICtm9YRkbgQ"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "S-3Bmvu1ihqA"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for model architecture\n",
    "# These parameters can be adjusted based on the size of the data and the specific requirements of the task\n",
    "\n",
    "n_layer = 6 # Number of layers in the transformer model\n",
    "n_head = 8 # Number of attention heads in each transformer layer\n",
    "n_emb = 512 # Dimensionality of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "OA1QEtv2lIX2"
   },
   "outputs": [],
   "source": [
    "# Use Hugging Face's AutoConfig to create a configuration for the GPT-2 model\n",
    "# The configuration is based on the \"gpt2\" pre-trained model, and some parameters are customized\n",
    "\n",
    "# Define the configuration using AutoConfig\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",                               # Base model: \"gpt2\"\n",
    "    vocab_size=len(tokenizer),            # Vocabulary size based on the tokenizer\n",
    "    n_positions=context_length,           # Maximum position embeddings (context length)\n",
    "    n_layer=n_layer,                      # Number of transformer layers\n",
    "    n_head=n_head,                        # Number of attention heads in each layer\n",
    "    pad_token_id=tokenizer.pad_token_id,  # ID of the padding token\n",
    "    bos_token_id=tokenizer.bos_token_id,  # ID of the beginning-of-sequence token\n",
    "    eos_token_id=tokenizer.eos_token_id,  # ID of the end-of-sequence token\n",
    "    n_embd=n_emb                           # Dimensionality of the embedding layer\n",
    ")\n",
    "\n",
    "# Create an instance of the GPT-2 language model using the configured parameters\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zfi_c7Ijmjav"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 20.1M parameters\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the size of the GPT-2 model in terms of parameters\n",
    "# The size is measured in millions (M) of parameters\n",
    "\n",
    "# Calculate the total number of parameters in the GPT-2 model\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "\n",
    "# Print the size of the GPT-2 model in millions of parameters\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBfWG2iZBLop"
   },
   "source": [
    "Create a datacollator to will take care of preparing the data:\n",
    "\n",
    "\"Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the DataCollatorForLanguageModeling collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don’t need to duplicate the input_ids.\n",
    "\n",
    "Note that DataCollatorForLanguageModeling supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument mlm=False:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "xGg8htNBnF_m"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "KToTnxF8Ckkn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 359])\n",
      "attention_mask shape: torch.Size([5, 359])\n",
      "labels shape: torch.Size([5, 359])\n",
      "Collated outputs: {'input_ids': tensor([[ 74,  24,  22,  ...,   3,   3,   3],\n",
      "        [ 74,  24,  22,  ...,   3,   3,   3],\n",
      "        [ 74,  24,  22,  ..., 111,   8,  23],\n",
      "        [ 74,  24,  22,  ...,   3,   3,   3],\n",
      "        [ 74,  24,  22,  ...,   3,   3,   3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  74,   24,   22,  ..., -100, -100, -100],\n",
      "        [  74,   24,   22,  ..., -100, -100, -100],\n",
      "        [  74,   24,   22,  ...,  111,    8,   23],\n",
      "        [  74,   24,   22,  ..., -100, -100, -100],\n",
      "        [  74,   24,   22,  ..., -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "# Test the data collator on a small subset of the training data\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "\n",
    "# Print the shapes of the collated outputs\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "# Display the collated outputs\n",
    "print(f\"Collated outputs: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "OLNX31-WDMcX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login into wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "8MpkHZ6qO_Zt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL='checkpoint'\n"
     ]
    }
   ],
   "source": [
    "# Set the WandB environment variable to log the model checkpoint\n",
    "%env WANDB_LOG_MODEL='checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lyOSwB1dJHcW"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584c0a77358a406e9bec25be2f95cc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login into Hugging Face\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Ry-BYiGMJiq2"
   },
   "outputs": [],
   "source": [
    "# Create the args for out trainer\n",
    "\n",
    "# Get the output directory with timestamp.\n",
    "output_path = \"output\"\n",
    "steps = 5000\n",
    "\n",
    "# Define training configuration parameters\n",
    "# Commented parameters correspond to the small model\n",
    "config = {\"output_dir\": output_path,\n",
    "          \"num_train_epochs\": 1,\n",
    "          \"per_device_train_batch_size\": 8,\n",
    "          \"per_device_eval_batch_size\": 4,\n",
    "          \"evaluation_strategy\": \"steps\",\n",
    "          \"save_strategy\": \"steps\",\n",
    "          \"eval_steps\": steps,\n",
    "          \"logging_steps\":steps,\n",
    "          \"logging_first_step\": True,\n",
    "          \"save_total_limit\": 5,\n",
    "          \"save_steps\": steps,\n",
    "          \"lr_scheduler_type\": \"cosine\",\n",
    "          \"learning_rate\":5e-4,\n",
    "          \"warmup_ratio\": 0.01,\n",
    "          \"weight_decay\": 0.01,\n",
    "          \"seed\": 1,\n",
    "          \"load_best_model_at_end\": True,\n",
    "          \"report_to\": \"wandb\"}\n",
    "\n",
    "# Create Namespace object with configuration\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "M07MnTwqJ34f"
   },
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Zby2k2tQNYFG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\naomi\\Thesis\\Thesis\\Thesis-main\\wandb\\run-20240120_213352-z5mroc4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/musicgen/pop909-pretokenization/runs/z5mroc4i' target=\"_blank\">vocal-water-1</a></strong> to <a href='https://wandb.ai/musicgen/pop909-pretokenization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/musicgen/pop909-pretokenization' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/musicgen/pop909-pretokenization/runs/z5mroc4i' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization/runs/z5mroc4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and start a new WandB run for training\n",
    "run = wandb.init(project=wandb_project, job_type=\"training\", config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Y-OMMfag8GEo"
   },
   "outputs": [],
   "source": [
    "# Code for converting token sequences to NoteSequences with audio-related information\n",
    "\n",
    "# Constants for note durations\n",
    "NOTE_LENGTH_16TH_120BPM = 0.25 * 60 / 120\n",
    "BAR_LENGTH_120BPM = 4.0 * 60 / 120\n",
    "\n",
    "def token_sequence_to_note_sequence(token_sequence, use_program=True, use_drums=True, instrument_mapper=None, only_piano=False):\n",
    "    \"\"\"\n",
    "    Convert a token sequence to a NoteSequence with audio-related information.\n",
    "    Args:\n",
    "        token_sequence (list or str): Token sequence representing musical information.\n",
    "        use_program (bool): Whether to use program information for instruments.\n",
    "        use_drums (bool): Whether to include drums in the output.\n",
    "        instrument_mapper (dict): Mapping of instrument names to MIDI program numbers.\n",
    "        only_piano (bool): Whether to include only piano instruments in the output.\n",
    "    Returns:\n",
    "        note_sequence (NoteSequence): Converted NoteSequence.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(token_sequence, str):\n",
    "        token_sequence = token_sequence.split()\n",
    "\n",
    "    note_sequence = empty_note_sequence()\n",
    "\n",
    "    # Render all notes.\n",
    "    current_program = 1\n",
    "    current_is_drum = False\n",
    "    current_instrument = 0\n",
    "    track_count = 0\n",
    "    for token_index, token in enumerate(token_sequence):\n",
    "\n",
    "        if token == \"PIECE_START\":\n",
    "            pass\n",
    "        elif token == \"PIECE_END\":\n",
    "            print(\"The end.\")\n",
    "            break\n",
    "        elif token == \"TRACK_START\":\n",
    "            current_bar_index = 0\n",
    "            track_count += 1\n",
    "            pass\n",
    "        elif token == \"TRACK_END\":\n",
    "            pass\n",
    "        elif token == \"KEYS_START\":\n",
    "            pass\n",
    "        elif token == \"KEYS_END\":\n",
    "            pass\n",
    "        elif token.startswith(\"KEY=\"):\n",
    "            pass\n",
    "        elif token.startswith(\"INST\"):\n",
    "            instrument = token.split(\"=\")[-1]\n",
    "            if instrument != \"DRUMS\" and use_program:\n",
    "                if instrument_mapper is not None:\n",
    "                    if instrument in instrument_mapper:\n",
    "                        instrument = instrument_mapper[instrument]\n",
    "                current_program = int(instrument)\n",
    "                current_instrument = track_count\n",
    "                current_is_drum = False\n",
    "            if instrument == \"DRUMS\" and use_drums:\n",
    "                current_instrument = 0\n",
    "                current_program = 0\n",
    "                current_is_drum = True\n",
    "        elif token == \"BAR_START\":\n",
    "            current_time = current_bar_index * BAR_LENGTH_120BPM\n",
    "            current_notes = {}\n",
    "        elif token == \"BAR_END\":\n",
    "            current_bar_index += 1\n",
    "            pass\n",
    "        elif token.startswith(\"NOTE_ON\"):\n",
    "            pitch = int(token.split(\"=\")[-1])\n",
    "            note = note_sequence.notes.add()\n",
    "            note.start_time = current_time\n",
    "            note.end_time = current_time + 4 * NOTE_LENGTH_16TH_120BPM\n",
    "            note.pitch = pitch\n",
    "            note.instrument = current_instrument\n",
    "            note.program = current_program\n",
    "            note.velocity = 80\n",
    "            note.is_drum = current_is_drum\n",
    "            current_notes[pitch] = note\n",
    "        elif token.startswith(\"NOTE_OFF\"):\n",
    "            pitch = int(token.split(\"=\")[-1])\n",
    "            if pitch in current_notes:\n",
    "                note = current_notes[pitch]\n",
    "                note.end_time = current_time\n",
    "        elif token.startswith(\"TIME_DELTA\"):\n",
    "            delta = float(token.split(\"=\")[-1]) * NOTE_LENGTH_16TH_120BPM\n",
    "            current_time += delta\n",
    "        elif token.startswith(\"DENSITY=\"):\n",
    "            pass\n",
    "        elif token == \"[PAD]\":\n",
    "            pass\n",
    "        else:\n",
    "            #print(f\"Ignored token {token}.\")\n",
    "            pass\n",
    "\n",
    "    # Make the instruments right.\n",
    "    instruments_drums = []\n",
    "    for note in note_sequence.notes:\n",
    "        pair = [note.program, note.is_drum]\n",
    "        if pair not in instruments_drums:\n",
    "            instruments_drums += [pair]\n",
    "        note.instrument = instruments_drums.index(pair)\n",
    "\n",
    "    if only_piano:\n",
    "        for note in note_sequence.notes:\n",
    "            if not note.is_drum:\n",
    "                note.instrument = 0\n",
    "                note.program = 0\n",
    "\n",
    "    return note_sequence\n",
    "\n",
    "def empty_note_sequence(qpm=120.0, total_time=0.0):\n",
    "    \"\"\"\n",
    "    Create an empty NoteSequence with specified tempo and total time.\n",
    "    Args:\n",
    "        qpm (float): Quarter notes per minute (tempo).\n",
    "        total_time (float): Total time of the NoteSequence.\n",
    "    Returns:\n",
    "        note_sequence (NoteSequence): Empty NoteSequence.\n",
    "    \"\"\"\n",
    "    note_sequence = note_seq.protobuf.music_pb2.NoteSequence()\n",
    "    note_sequence.tempos.add().qpm = qpm\n",
    "    note_sequence.ticks_per_quarter = note_seq.constants.STANDARD_PPQ\n",
    "    note_sequence.total_time = total_time\n",
    "    return note_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "CYdVIUZ_7xWR"
   },
   "outputs": [],
   "source": [
    "# first create a custom trainer to log prediction distribution\n",
    "SAMPLE_RATE=44100\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader,\n",
    "        description,\n",
    "        prediction_loss_only=None,\n",
    "        ignore_keys=None,\n",
    "        metric_key_prefix=\"eval\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom evaluation loop to log prediction distribution.\n",
    "        Args:\n",
    "            dataloader: DataLoader for evaluation data.\n",
    "            description: Description of the evaluation.\n",
    "            prediction_loss_only: If True, only calculate prediction loss.\n",
    "            ignore_keys: List of keys to ignore.\n",
    "            metric_key_prefix: Prefix for metric keys.\n",
    "        Returns:\n",
    "            eval_output: Evaluation output.\n",
    "        \"\"\"\n",
    "        # Call super class method to get the eval outputs\n",
    "        eval_output = super().evaluation_loop(\n",
    "            dataloader,\n",
    "            description,\n",
    "            prediction_loss_only,\n",
    "            ignore_keys,\n",
    "            metric_key_prefix,\n",
    "        )\n",
    "\n",
    "        # Log the prediction distribution using `wandb.Histogram` method.\n",
    "        if wandb.run is not None:\n",
    "            input_ids = tokenizer.encode(\"PIECE_START STYLE=JSFAKES GENRE=JSFAKES TRACK_START\", return_tensors=\"pt\").cuda()\n",
    "            # Generate more tokens.\n",
    "            voice1_generated_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_length=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.75,\n",
    "                eos_token_id=tokenizer.encode(\"TRACK_END\")[0]\n",
    "            )\n",
    "            voice2_generated_ids = model.generate(\n",
    "                voice1_generated_ids,\n",
    "                max_length=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.75,\n",
    "                eos_token_id=tokenizer.encode(\"TRACK_END\")[0]\n",
    "            )\n",
    "            voice3_generated_ids = model.generate(\n",
    "                voice2_generated_ids,\n",
    "                max_length=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.75,\n",
    "                eos_token_id=tokenizer.encode(\"TRACK_END\")[0]\n",
    "            )\n",
    "            voice4_generated_ids = model.generate(\n",
    "                voice3_generated_ids,\n",
    "                max_length=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.75,\n",
    "                eos_token_id=tokenizer.encode(\"TRACK_END\")[0]\n",
    "            )\n",
    "            token_sequence = tokenizer.decode(voice4_generated_ids[0])\n",
    "            note_sequence = token_sequence_to_note_sequence(token_sequence)\n",
    "            synth = note_seq.fluidsynth\n",
    "            array_of_floats = synth(note_sequence, sample_rate=SAMPLE_RATE)\n",
    "            int16_data = note_seq.audio_io.float_samples_to_int16(array_of_floats)\n",
    "            wandb.log({\"Generated_audio\": wandb.Audio(int16_data, SAMPLE_RATE)})\n",
    "\n",
    "\n",
    "        return eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "JVsTKx3t-hLP"
   },
   "outputs": [],
   "source": [
    "# Create TrainingArguments object with training configuration \n",
    "train_args = TrainingArguments(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "D9hBshTJ9coM"
   },
   "outputs": [],
   "source": [
    "# Create a CustomTrainer instance for model training\n",
    "\n",
    "# Initialize a CustomTrainer with the specified parameters\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SEgCmFiz9v6S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='3742' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  22/3742 09:32 < 29:35:03, 0.03 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1843\u001b[0m ):\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2685\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2706\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2707\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2708\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2709\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1077\u001b[0m     input_ids,\n\u001b[0;32m   1078\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1079\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1080\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1081\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1082\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1083\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1084\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1085\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1086\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1087\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1088\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1089\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    892\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    898\u001b[0m     )\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[0;32m    901\u001b[0m         hidden_states,\n\u001b[0;32m    902\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m    903\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    904\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[0;32m    905\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    906\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    907\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    908\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    909\u001b[0m     )\n\u001b[0;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m--> 354\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m    355\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m    356\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:105\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    104\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[1;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39maddmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iomIVDkb8PeZ"
   },
   "outputs": [],
   "source": [
    "# call wandb.finish() to finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPkkleKq-zzL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
