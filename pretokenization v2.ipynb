{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6c05ae",
   "metadata": {},
   "source": [
    "reference: https://colab.research.google.com/drive/1876dq54hRsWGWdsrC6E4cYdxmacb5a4S?usp=sharing#scrollTo=KDKaWJI2aAxZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527351ec",
   "metadata": {},
   "source": [
    "**Summary** <br>\n",
    "* We use the tokenizers library to create a tokenizer and train it on a sample text. <br>\n",
    "* The trained tokenizer is saved to a file (tokenizer.json) and then loaded back.<br>\n",
    "* The sample text is tokenized using the loaded tokenizer.<br>\n",
    "* A DataFrame is created from the tokens.<br>\n",
    "* We use WandB to log the vocabulary DataFrame as an artifact.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60436e56",
   "metadata": {},
   "source": [
    "\"As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That's where the pre-tokenization step comes in. As we saw in Chapter 2, a word-based tokenizer can simply split a raw text into words on whitespace and punctuation.\" HF course. <br>\n",
    "\n",
    "In our case, this step is really simple, we need our pretokenization to split our text in \"words\" since our dataset is already a series of tokens. So a Whitespace pre_tokenizer would work fine here. The model we will use is, again, \"WordLevel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d71cd1",
   "metadata": {},
   "source": [
    "transfo-xl â€” TransfoXLConfig (Transformer-XL model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c6a660",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (4.21.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
      "     -------------------------------------- 254.1/254.1 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (63.4.1)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "     -------------------------------------- 190.6/190.6 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (4.9.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-win_amd64.whl (11 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\naomi\\appdata\\roaming\\python\\python39\\site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\naomi\\appdata\\roaming\\python\\python39\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\naomi\\appdata\\roaming\\python\\python39\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\naomi\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033d79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers, models, pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72aeed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIECE_START TRACK_START INST=0 BAR_START TIME_DELTA=24 NOTE_ON=70 NOTE_ON=59 NOTE_ON=63 NOTE_ON=47 TIME_DELTA=3 NOTE_OFF=70 TIME_DELTA=1 NOTE_ON=78 TIME_DELTA=3 NOTE_OFF=78 TIME_DELTA=2 NOTE_ON=66 NOTE_ON=80 NOTE_ON=54 NOTE_OFF=59 TIME_DELTA=\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'vocab': failed to extract enum PyVocab ('Vocab | Filename')\n- variant Vocab (Vocab): TypeError: failed to extract field PyVocab::Vocab.0, caused by TypeError: 'list' object cannot be converted to 'PyDict'\n- variant Filename (Filename): TypeError: failed to extract field PyVocab::Filename.0, caused by TypeError: 'list' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create a new WordLevel tokenizer with vocabulary including [UNK]\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m new_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWordLevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[UNK]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize Tokenizer\u001b[39;00m\n\u001b[0;32m     24\u001b[0m new_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(models\u001b[38;5;241m.\u001b[39mWordLevel())\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 'vocab': failed to extract enum PyVocab ('Vocab | Filename')\n- variant Vocab (Vocab): TypeError: failed to extract field PyVocab::Vocab.0, caused by TypeError: 'list' object cannot be converted to 'PyDict'\n- variant Filename (Filename): TypeError: failed to extract field PyVocab::Filename.0, caused by TypeError: 'list' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"C:/Users/naomi/Thesis/Thesis/Thesis-main/tokenized_output_v2/all_tokenized_outputs.txt\"\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    all_tokenized_outputs = file.readlines()\n",
    "\n",
    "# Calculate the index for the 10% split\n",
    "split_index = int(0.1 * len(all_tokenized_outputs))\n",
    "\n",
    "# Extract the 11th sample (index 10)\n",
    "sample_10 = all_tokenized_outputs[10]\n",
    "\n",
    "# Take the first 242 characters of the sample\n",
    "sample = sample_10[:242]\n",
    "\n",
    "# Output the content of the sample variable\n",
    "print(sample)\n",
    "\n",
    "# Create a new WordLevel tokenizer with vocabulary including [UNK]\n",
    "new_tokenizer = Tokenizer(models.WordLevel(vocab=[\"[UNK]\"]))\n",
    "\n",
    "# Initialize Tokenizer\n",
    "new_tokenizer = Tokenizer(models.WordLevel())\n",
    "\n",
    "# Add pretokenizer\n",
    "new_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "\n",
    "# Yield batches of 1,000 texts\n",
    "def get_training_corpus():\n",
    "    dataset = all_tokenized_outputs  # Use all_tokenized_outputs directly\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000]\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d520c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer\n",
    "new_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec3c76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained tokenizer\n",
    "new_tokenizer.save(\"trained_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d4995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "loaded_tokenizer = Tokenizer.from_file(\"trained_tokenizer.json\")\n",
    "loaded_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "149e880f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "WordLevel error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the sample text using the loaded tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m encoded\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[1;31mException\u001b[0m: WordLevel error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "# Tokenize the sample text using the loaded tokenizer\n",
    "encoded = loaded_tokenizer.encode(sample)\n",
    "tokens = encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da180f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the vocabulary\n",
    "vocab_df = pd.DataFrame(\n",
    "    [{\"Token\": token, \"Index\": idx} for idx, token in enumerate(tokens)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B run\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with vocab\n",
    "vocab_table = wandb.Table(data=vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artifact for raw data\n",
    "processed_data_at = wandb.Artifact(name=\"processed_data\", type=\"processed_data\")\n",
    "processed_data_at.add(vocab_table, name=\"vocab_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7335bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the artifact\n",
    "wandb.log_artifact(processed_data_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884cbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a63db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
