{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, load, device, set_grad_enabled, max, sum, cat\n",
    "from preprocessing.nn_dataset import get_test_set_for_eval_classic\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "import note_seq\n",
    "\n",
    "\"\"\"\n",
    "File containing functions to quantitatively evaluate trained models\n",
    "\"\"\"\n",
    "\n",
    "def eval_on_test_set(load_path, model, criterion, set='test', notes_only=False, tokenizer=None):\n",
    "    model = model\n",
    "\n",
    "    try:\n",
    "        if cuda.is_available():\n",
    "            model.load_state_dict(load(load_path)['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(load(load_path, map_location=device('cpu'))['model_state_dict'])\n",
    "        print(\"loaded params from\", load_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f'No file located at {load_path}, could not load parameters')\n",
    "    print(model)\n",
    "\n",
    "    if cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "    criterion = criterion\n",
    "\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "    loss_epoch = 0\n",
    "    running_accuracy = 0.0\n",
    "    running_batch_count = 0\n",
    "    print_loss_batch = 0  # Reset on print\n",
    "    print_acc_batch = 0  # Reset on print\n",
    "    pr_interval = 1\n",
    "\n",
    "    for x, y, psx, i, c in get_test_set_for_eval_classic(set):\n",
    "        model.zero_grad()\n",
    "\n",
    "        train_emb = False\n",
    "\n",
    "        Y = y\n",
    "\n",
    "        with set_grad_enabled(False):\n",
    "            # Tokenize input sequence using the specified tokenizer\n",
    "            input_ids = tokenizer.encode(x, return_tensors=\"pt\").cuda()\n",
    "\n",
    "            # Generate predictions from the model\n",
    "            y_hat = model.generate(\n",
    "                input_ids,\n",
    "                max_length=2048,\n",
    "                do_sample=True,\n",
    "                temperature=0.75,\n",
    "                eos_token_id=tokenizer.encode(\"TRACK_END\")[0]\n",
    "            )\n",
    "\n",
    "            # Decode the generated tokens into a token sequence\n",
    "            token_sequence = tokenizer.decode(y_hat[0])\n",
    "\n",
    "            # Convert the token sequence into a NoteSequence\n",
    "            generated_note_sequence = token_sequence_to_note_sequence(token_sequence)\n",
    "\n",
    "            if notes_only:\n",
    "                # Extract only note-related information if specified\n",
    "                # (you may need to modify this part based on your specific task)\n",
    "                # ...\n",
    "                pass\n",
    "            else:\n",
    "                # Process the entire generated NoteSequence\n",
    "                # (you may need to modify this part based on your specific task)\n",
    "                # ...\n",
    "                pass\n",
    "\n",
    "            # Calculate the loss using the criterion\n",
    "            loss = criterion(generated_note_sequence, Y)\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "        print_loss_batch += loss.item()\n",
    "\n",
    "        if notes_only:\n",
    "            # Calculate accuracy for note-related information if specified\n",
    "            # (you may need to modify this part based on your specific task)\n",
    "            # ...\n",
    "            pass\n",
    "        else:\n",
    "            # Calculate accuracy for the entire generated sequence\n",
    "            # (you may need to modify this part based on your specific task)\n",
    "            # ...\n",
    "            pass\n",
    "\n",
    "        count += 1\n",
    "        batch_count += len(x)\n",
    "        running_batch_count += len(x)\n",
    "\n",
    "        # print loss for recent set of batches\n",
    "        if count % pr_interval == 0:\n",
    "            ave_loss = print_loss_batch / pr_interval\n",
    "            ave_acc = 100 * print_acc_batch.float() / running_batch_count\n",
    "            print_acc_batch = 0\n",
    "            running_batch_count = 0\n",
    "            print('\\t\\t[%d] loss: %.3f, acc: %.3f' % (count, ave_loss, ave_acc))\n",
    "            print_loss_batch = 0\n",
    "\n",
    "    # calculate loss and accuracy for phase\n",
    "    ave_loss_epoch = loss_epoch / count\n",
    "    epoch_acc = 100 * running_accuracy.float() / batch_count\n",
    "    print('\\tfinished %s phase loss: %.3f, acc: %.3f' % ('eval', ave_loss_epoch, epoch_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24391fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume this function is present in your second snippet\n",
    "def token_sequence_to_note_sequence(token_sequence):\n",
    "    # Implement this function based on your specific token-to-note conversion logic\n",
    "    # This function should convert the token sequence into a NoteSequence\n",
    "    # ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "eval_on_test_set('path_to_model_checkpoint.pth', your_model_instance, your_criterion_instance, set='test', notes_only=False, tokenizer=your_tokenizer_instance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
