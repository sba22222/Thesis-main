{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display NVIDIA GPU information\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yPGNyAscsvzD"
   },
   "outputs": [],
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "# !pip install datasets transformers[torch]\n",
    "# !pip install wandb\n",
    "# !pip install note_seq\n",
    "# !pip install evaluate\n",
    "# !pip install -qU pyfluidsynth\n",
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the code is running in Google Colab environment\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Inform the user about installing dependencies in Colab\n",
    "    print(\"Installing dependencies...\")\n",
    "\n",
    "    # Update package information and install necessary dependencies using apt-get\n",
    "    !apt-get update -qq && apt-get install -qq libfluidsynth2 build-essential libasound2-dev libjack-dev\n",
    "\n",
    "    # Install the pyfluidsynth library using pip\n",
    "    !pip install -qU pyfluidsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naomi\\anaconda3\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and modules\n",
    "\n",
    "import os  # Operating system library for interacting with the file system\n",
    "import wandb  # Library for experiment tracking and visualization\n",
    "from huggingface_hub import notebook_login  # Log in to the Hugging Face Hub from a notebook\n",
    "import note_seq  # Music generation library\n",
    "\n",
    "from argparse import Namespace  # Namespace class for organizing command-line arguments\n",
    "\n",
    "from datasets import load_dataset  # Load datasets for training and evaluation\n",
    "from transformers import AutoTokenizer  # AutoTokenizer for automatically selecting tokenization method\n",
    "from transformers import DataCollatorForLanguageModeling  # Data collator for language modeling\n",
    "from transformers import set_seed  # Set seed for reproducibility\n",
    "from transformers import Trainer, TrainingArguments  # Trainer and TrainingArguments for model training\n",
    "\n",
    "import evaluate  # Custom module for evaluation (assuming it's part of the project)\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "\n",
    "from transformers import AutoConfig, GPT2LMHeadModel  # AutoConfig and GPT2LMHeadModel for GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "COgCtDY2bJpQ"
   },
   "outputs": [],
   "source": [
    "# Set the Protocol Buffers Python implementation to \"python\"\n",
    "# This line is used to resolve compatibility issues related to Protocol Buffers (protobuf)\n",
    "# It explicitly selects the pure Python implementation of the protobuf library\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q4gRrvsaPFrK"
   },
   "outputs": [],
   "source": [
    "# Set parameters for WandB (Weights & Biases) integration\n",
    "wandb_project = \"pop909_protokenization\"\n",
    "entity = \"musicgen\"\n",
    "data_processed = \"pop909_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8MpkHZ6qO_Zt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL='checkpoint'\n"
     ]
    }
   ],
   "source": [
    "# This line sets the environment variable WANDB_LOG_MODEL to 'checkpoint',\n",
    "# which configures WandB to log the model checkpoints during training.\n",
    "%env WANDB_LOG_MODEL='checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OLNX31-WDMcX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login into wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i0AqFqXavrBw"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5a9d7d327e4814adc8a803e5d61c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log in to the Hugging Face Hub from this notebook\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y-OMMfag8GEo"
   },
   "outputs": [],
   "source": [
    "# Code to log also some audio in the raw data\n",
    "\n",
    "NOTE_LENGTH_16TH_120BPM = 0.25 * 60 / 120\n",
    "BAR_LENGTH_120BPM = 4.0 * 60 / 120\n",
    "\n",
    "def token_sequence_to_note_sequence(token_sequence, use_program=True, use_drums=True, instrument_mapper=None, only_piano=False):\n",
    "    \"\"\"\n",
    "    Convert a token sequence to a NoteSequence.\n",
    "\n",
    "    Args:\n",
    "    - token_sequence: List of tokens representing a musical sequence.\n",
    "    - use_program: Flag to use program information for non-drum instruments.\n",
    "    - use_drums: Flag to use drums information.\n",
    "    - instrument_mapper: Dictionary to map instrument names.\n",
    "    - only_piano: Flag to keep only piano instruments.\n",
    "\n",
    "    Returns:\n",
    "    A NoteSequence object representing the musical sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(token_sequence, str):\n",
    "        token_sequence = token_sequence.split()\n",
    "\n",
    "    note_sequence = empty_note_sequence()\n",
    "\n",
    "    # Render all notes.\n",
    "    current_program = 1\n",
    "    current_is_drum = False\n",
    "    current_instrument = 0\n",
    "    track_count = 0\n",
    "    for token_index, token in enumerate(token_sequence):\n",
    "\n",
    "        if token == \"PIECE_START\":\n",
    "            pass\n",
    "        elif token == \"PIECE_END\":\n",
    "            print(\"The end.\")\n",
    "            break\n",
    "        elif token == \"TRACK_START\":\n",
    "            current_bar_index = 0\n",
    "            track_count += 1\n",
    "            pass\n",
    "        elif token == \"TRACK_END\":\n",
    "            pass\n",
    "        elif token == \"KEYS_START\":\n",
    "            pass\n",
    "        elif token == \"KEYS_END\":\n",
    "            pass\n",
    "        elif token.startswith(\"KEY=\"):\n",
    "            pass\n",
    "        elif token.startswith(\"INST\"):\n",
    "            instrument = token.split(\"=\")[-1]\n",
    "            if instrument != \"DRUMS\" and use_program:\n",
    "                if instrument_mapper is not None:\n",
    "                    if instrument in instrument_mapper:\n",
    "                        instrument = instrument_mapper[instrument]\n",
    "                current_program = int(instrument)\n",
    "                current_instrument = track_count\n",
    "                current_is_drum = False\n",
    "            if instrument == \"DRUMS\" and use_drums:\n",
    "                current_instrument = 0\n",
    "                current_program = 0\n",
    "                current_is_drum = True\n",
    "        elif token == \"BAR_START\":\n",
    "            current_time = current_bar_index * BAR_LENGTH_120BPM\n",
    "            current_notes = {}\n",
    "        elif token == \"BAR_END\":\n",
    "            current_bar_index += 1\n",
    "            pass\n",
    "        elif token.startswith(\"NOTE_ON\"):\n",
    "            pitch = int(token.split(\"=\")[-1])\n",
    "            note = note_sequence.notes.add()\n",
    "            note.start_time = current_time\n",
    "            note.end_time = current_time + 4 * NOTE_LENGTH_16TH_120BPM\n",
    "            note.pitch = pitch\n",
    "            note.instrument = current_instrument\n",
    "            note.program = current_program\n",
    "            note.velocity = 80\n",
    "            note.is_drum = current_is_drum\n",
    "            current_notes[pitch] = note\n",
    "        elif token.startswith(\"NOTE_OFF\"):\n",
    "            pitch = int(token.split(\"=\")[-1])\n",
    "            if pitch in current_notes:\n",
    "                note = current_notes[pitch]\n",
    "                note.end_time = current_time\n",
    "        elif token.startswith(\"TIME_DELTA\"):\n",
    "            delta = float(token.split(\"=\")[-1]) * NOTE_LENGTH_16TH_120BPM\n",
    "            current_time += delta\n",
    "        elif token.startswith(\"DENSITY=\"):\n",
    "            pass\n",
    "        elif token == \"[PAD]\":\n",
    "            pass\n",
    "        else:\n",
    "            #print(f\"Ignored token {token}.\")\n",
    "            pass\n",
    "\n",
    "    # Make the instruments right.\n",
    "    instruments_drums = []\n",
    "    for note in note_sequence.notes:\n",
    "        pair = [note.program, note.is_drum]\n",
    "        if pair not in instruments_drums:\n",
    "            instruments_drums += [pair]\n",
    "        note.instrument = instruments_drums.index(pair)\n",
    "\n",
    "    if only_piano:\n",
    "        for note in note_sequence.notes:\n",
    "            if not note.is_drum:\n",
    "                note.instrument = 0\n",
    "                note.program = 0\n",
    "\n",
    "    return note_sequence\n",
    "\n",
    "def empty_note_sequence(qpm=120.0, total_time=0.0):\n",
    "    \"\"\"\n",
    "    Create an empty NoteSequence with specified tempo and total time.\n",
    "\n",
    "    Args:\n",
    "    - qpm: Quarter notes per minute (tempo).\n",
    "    - total_time: Total time of the sequence.\n",
    "\n",
    "    Returns:\n",
    "    An empty NoteSequence object.\n",
    "    \"\"\"\n",
    "    note_sequence = note_seq.protobuf.music_pb2.NoteSequence()\n",
    "    note_sequence.tempos.add().qpm = qpm\n",
    "    note_sequence.ticks_per_quarter = note_seq.constants.STANDARD_PPQ\n",
    "    note_sequence.total_time = total_time\n",
    "    return note_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4vQF0ZdtPzP"
   },
   "source": [
    "## Download Dataset and tokenizer from Hugging Face\n",
    "\n",
    "In the previos notebook, we trained a tokenizer. We'll use it here first to do some basic EDA to understand our data and what type of model size is better (number of layers, heads, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CYdVIUZ_7xWR"
   },
   "outputs": [],
   "source": [
    "# first create a custom trainer to log prediction distribution\n",
    "# Set the sample rate for audio processing\n",
    "SAMPLE_RATE=44100\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader,\n",
    "        description,\n",
    "        prediction_loss_only=None,\n",
    "        ignore_keys=None,\n",
    "        metric_key_prefix=\"eval\",\n",
    "    ):\n",
    "        # Call super class method to get the eval outputs\n",
    "        eval_output = super().evaluation_loop(\n",
    "            dataloader,\n",
    "            description,\n",
    "            prediction_loss_only,\n",
    "            ignore_keys,\n",
    "            metric_key_prefix,\n",
    "        )\n",
    "\n",
    "         # Log the prediction distribution using `wandb.Histogram` method.\n",
    "        if wandb.run is not None:\n",
    "            # Encode a starting token to begin the generation\n",
    "            input_ids = self.tokenizer.encode(\"PIECE_START\", return_tensors=\"pt\").cuda()\n",
    "\n",
    "            # Generate more tokens for each voice\n",
    "            for voice_num in range(1, 5):\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=2048,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.75,\n",
    "                    eos_token_id=self.tokenizer.encode(\"TRACK_END\")[0]\n",
    "                )\n",
    "\n",
    "                # Decode the generated tokens into a token sequence\n",
    "                token_sequence = self.tokenizer.decode(generated_ids[0])\n",
    "                \n",
    "                # Convert the token sequence into a NoteSequence\n",
    "                note_sequence = token_sequence_to_note_sequence(token_sequence)\n",
    "\n",
    "                # Synthesize the audio from the NoteSequence\n",
    "                synth = note_seq.fluidsynth\n",
    "                array_of_floats = synth(note_sequence, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "                # Convert the float audio samples to int16 format\n",
    "                int16_data = note_seq.audio_io.float_samples_to_int16(array_of_floats)\n",
    "\n",
    "                # Log the generated audio using the wandb.Audio method\n",
    "                wandb.log({\"Generated_audio_voice_\" + str(voice_num): wandb.Audio(int16_data, SAMPLE_RATE)})\n",
    "\n",
    "        # Return the evaluation output\n",
    "        return eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H5uNGnLak-bO"
   },
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ry-BYiGMJiq2"
   },
   "outputs": [],
   "source": [
    "# Configuration parameters for the training process\n",
    "# Commented parameters correspond to the small model\n",
    "\n",
    "config = {\n",
    "    \"output_dir\": \"output\",  # Directory to save the model and training outputs\n",
    "    \"num_train_epochs\": 1,   # Number of training epochs\n",
    "    \"per_device_train_batch_size\": 4,  # Batch size for training on each device\n",
    "    \"per_device_eval_batch_size\": 2,   # Batch size for evaluation on each device\n",
    "    \"evaluation_strategy\": \"steps\",   # Evaluation strategy during training\n",
    "    \"save_strategy\": \"steps\",         # Save strategy during training\n",
    "    \"eval_steps\": 2497,               # Number of steps before evaluation\n",
    "    \"logging_steps\": 2497,            # Number of steps before logging\n",
    "    \"logging_first_step\": True,       # Log metrics on the first training step\n",
    "    \"save_total_limit\": 2,            # Limit on the total number of checkpoints to save\n",
    "    \"save_steps\": 2497,               # Number of steps before saving a checkpoint\n",
    "    \"lr_scheduler_type\": \"cosine\",    # Learning rate scheduler type (cosine)\n",
    "    \"learning_rate\": 5e-4,            # Initial learning rate\n",
    "    \"warmup_ratio\": 0.01,             # Ratio of warmup steps during learning rate warmup\n",
    "    \"weight_decay\": 0.01,             # Weight decay for optimization\n",
    "    \"seed\": 1,                        # Random seed for reproducibility\n",
    "    \"load_best_model_at_end\": True,   # Load the best model at the end of training\n",
    "    \"report_to\": \"wandb\",            # Reporting to WandB (Weights and Biases)\n",
    "    \"prediction_loss_only\": False,    # Whether to compute only the prediction loss during training\n",
    "    \"gradient_accumulation_steps\": 1  # Number of steps for gradient accumulation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zlYMDsA1A30P"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_fn(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the model predictions.\n",
    "\n",
    "    Args:\n",
    "    - eval_pred: The output of the model during evaluation.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    metrics = dict()\n",
    "\n",
    "    # Load the accuracy metric from the 'evaluate' module\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    # Extract predictions and labels from the evaluation predictions\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # Filter out padding tokens (tokens with label -100)\n",
    "    not_pad_mask = labels != -100\n",
    "    logits, labels = logits[not_pad_mask], labels[not_pad_mask]\n",
    "\n",
    "    # Compute predictions by taking the argmax along the last dimension\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Flatten the predictions and labels to match the expected format\n",
    "    flat_predictions = predictions.flatten()  # Shape: (num_samples * sequence_length,)\n",
    "    flat_labels = labels.flatten()  # Shape: (num_samples * sequence_length,)\n",
    "\n",
    "    # Compute accuracy metric and add it to the metrics dictionary\n",
    "    metrics.update(\n",
    "        accuracy_metric.compute(references=flat_labels, predictions=flat_predictions)\n",
    "    )\n",
    "\n",
    "    # Return the computed metrics\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3Se37G7o30ap"
   },
   "outputs": [],
   "source": [
    "def get_raw_data_and_tokenizer():\n",
    "    \"\"\"\n",
    "    Load raw data and tokenizer for the MMM Track LMD 8-bars dataset.\n",
    "\n",
    "    Returns:\n",
    "    - raw_datasets: A Hugging Face Dataset containing the raw data split into training and testing sets.\n",
    "    - tokenizer: A Hugging Face AutoTokenizer for tokenizing the dataset.\n",
    "    \"\"\"\n",
    "    # Load the MMM Track LMD 8-bars dataset from Hugging Face's datasets library\n",
    "    ds = load_dataset(\"aimusicgen/pop909_clean_data\", split=\"train\")\n",
    "\n",
    "    # Split the dataset into training and testing sets with 10% for testing and shuffle the data\n",
    "    raw_datasets = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "    # Change the tokenizer based on the specific model or task requirements\n",
    "    # In this case, it's using a tokenizer from Hugging Face's model hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"aimusicgen/pop909_tokenizer\")\n",
    "\n",
    "    # Return the raw datasets and the tokenizer\n",
    "    return raw_datasets, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cCxK_DEBREOs"
   },
   "outputs": [],
   "source": [
    "def tokenize(element, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize a given element using a specified tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - element: The input element to be tokenized.\n",
    "    - tokenizer: The tokenizer to be used for tokenization.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the tokenized representation of the input element.\n",
    "    \"\"\"\n",
    "    # Replace this based on Dataset\n",
    "    context_length = CONTEXT_LENGTH\n",
    "\n",
    "    # Use the tokenizer to tokenize the text in the input element\n",
    "    # Truncate long elements, but no effect in JSB\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    # Return a dictionary containing the tokenized input IDs\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4zunRo-CfEHr"
   },
   "outputs": [],
   "source": [
    "def create_tokenized_dataset(raw_datasets, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize the raw datasets using a specified tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - raw_datasets: The raw datasets to be tokenized.\n",
    "    - tokenizer: The tokenizer to be used for tokenization.\n",
    "\n",
    "    Returns:\n",
    "    A tokenized version of the input datasets.\n",
    "    \"\"\"\n",
    "    # Create a tokenized dataset using the 'tokenize' function on each batch\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )\n",
    "\n",
    "    # Return the tokenized datasets\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S-3Bmvu1ihqA"
   },
   "outputs": [],
   "source": [
    "def create_model(tokenizer):\n",
    "    \"\"\"\n",
    "    Create a GPT-2 language model for the specified tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - tokenizer: The tokenizer for tokenizing the input.\n",
    "\n",
    "    Returns:\n",
    "    A GPT-2 language model.\n",
    "    \"\"\"\n",
    "    # Change these values based on the size of the data\n",
    "    n_layer = 6\n",
    "    n_head = 8\n",
    "    n_emb = 512\n",
    "\n",
    "    # Create a configuration for the GPT-2 model\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        vocab_size=len(tokenizer),\n",
    "        n_positions=CONTEXT_LENGTH,\n",
    "        n_layer=n_layer,\n",
    "        n_head=n_head,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        n_embd=n_emb\n",
    "    )\n",
    "\n",
    "    # Instantiate the GPT-2 language model using the specified configuration\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    # Return the created GPT-2 model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xGg8htNBnF_m"
   },
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    \"\"\"\n",
    "    Train a language model based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "    - config: Configuration parameters for training.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Set a seed for reproducibility\n",
    "    set_seed(config[\"seed\"])\n",
    "\n",
    "    # Load raw data and tokenizer\n",
    "    raw_datasets, tokenizer = get_raw_data_and_tokenizer()\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_datasets = create_tokenized_dataset(raw_datasets=raw_datasets, tokenizer=tokenizer)\n",
    "\n",
    "    # Create the GPT-2 language model\n",
    "    model = create_model(tokenizer)\n",
    "\n",
    "    # Initialize WandB for logging and monitoring\n",
    "    run = wandb.init(project=wandb_project, job_type=\"training\", config=config)\n",
    "\n",
    "    # Create a data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    # Create training arguments\n",
    "    train_args = TrainingArguments(**config)\n",
    "\n",
    "    # Initialize the custom trainer for training the model\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        # compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Finish logging with WandB\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "b01Lr1LEqzfE"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d53e292e969421090f2caf0b3454320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to C:/Users/naomi/.cache/huggingface/datasets/juancopi81___parquet/juancopi81--mmm_track_lmd_8bars_nots-449984f4d6dfe7ce/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b8d85e074b48cd94dd6a1415f19c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4501dfd2ff4d46b571e59fa5b086d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/76.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babedbc338ac4a0792034f3f9da774f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/70.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f40073d787048d38bd0e59912145748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/68.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f45c40a41843f9b06d99e63370777a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d863a26a2034262b886db0bed8fb56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/69.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b8a0be36aa4237aae370f13cb7ef11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/67.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ddcf5fb5204bfd9eb766fc8076b88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/69.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fbca5714fa44eb8a8e452af7b9ccf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/177567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/naomi/.cache/huggingface/datasets/juancopi81___parquet/juancopi81--mmm_track_lmd_8bars_nots-449984f4d6dfe7ce/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30496ff1db3c40168da201e78b12b99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naomi\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\naomi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cad87369d54ae4896c73d3c59a2f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/602k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9cf69b94664a4a9c7520d695ec5e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(config)\n",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m      4\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m create_tokenized_dataset(raw_datasets\u001b[38;5;241m=\u001b[39mraw_datasets, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(tokenizer)\n\u001b[1;32m----> 6\u001b[0m run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mWANDB_PROJECT, job_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m      7\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m train_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "# Trigger the training process with the specified configuration\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3dMnBcZsAtD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
