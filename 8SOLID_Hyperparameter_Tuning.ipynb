{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3lJKe6TGC8R",
    "outputId": "5133b1d8-2cc1-401b-eab6-f53160eb2fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 24 13:12:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gsfH-EU7KPi",
    "outputId": "c87a15e8-e6e1-45ea-a0cc-a627b41ffd22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Thesis-main'...\n",
      "remote: Enumerating objects: 25765, done.\u001b[K\n",
      "remote: Counting objects: 100% (6564/6564), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5756/5756), done.\u001b[K\n",
      "remote: Total 25765 (delta 88), reused 6540 (delta 66), pack-reused 19201\u001b[K\n",
      "Receiving objects: 100% (25765/25765), 79.60 MiB | 15.76 MiB/s, done.\n",
      "Resolving deltas: 100% (423/423), done.\n",
      "Updating files: 100% (36302/36302), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://sba22222@github.com/sba22222/Thesis-main\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbO3hmaF7T2e",
    "outputId": "2e166090-1b85-46c4-a690-d7313a1937c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/507.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/507.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: dill, responses, multiprocess, accelerate, datasets, evaluate\n",
      "Successfully installed accelerate-0.26.1 datasets-2.16.1 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 responses-0.18.0\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.2 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n",
      "Collecting note_seq\n",
      "  Downloading note_seq-0.0.5-py3-none-any.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from note_seq) (1.4.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from note_seq) (23.2.0)\n",
      "Requirement already satisfied: bokeh>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from note_seq) (3.3.3)\n",
      "Collecting intervaltree>=2.1.0 (from note_seq)\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from note_seq) (7.34.0)\n",
      "Requirement already satisfied: librosa>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from note_seq) (0.10.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from note_seq) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from note_seq) (1.5.3)\n",
      "Collecting pretty-midi>=0.2.6 (from note_seq)\n",
      "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting protobuf>=4.21.2 (from note_seq)\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydub (from note_seq)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from note_seq) (1.11.4)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh>=0.12.0->note_seq) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=0.12.0->note_seq) (1.2.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh>=0.12.0->note_seq) (23.2)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=0.12.0->note_seq) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh>=0.12.0->note_seq) (6.0.1)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=0.12.0->note_seq) (6.3.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=0.12.0->note_seq) (2023.10.1)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=2.1.0->note_seq) (2.4.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (0.58.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.2->note_seq) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18.1->note_seq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18.1->note_seq) (2023.3.post1)\n",
      "Collecting mido>=1.1.16 (from pretty-midi>=0.2.6->note_seq)\n",
      "  Downloading mido-1.3.2-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty-midi>=0.2.6->note_seq) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (67.7.2)\n",
      "Collecting jedi>=0.16 (from IPython->note_seq)\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (3.0.43)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (2.16.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->note_seq) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->note_seq) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh>=0.12.0->note_seq) (2.1.3)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.2->note_seq) (0.41.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->note_seq) (0.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.6.2->note_seq) (4.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.6.2->note_seq) (2.31.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->note_seq) (0.2.13)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa>=0.6.2->note_seq) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa>=0.6.2->note_seq) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.6.2->note_seq) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.2->note_seq) (2023.11.17)\n",
      "Building wheels for collected packages: intervaltree, pretty-midi\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26095 sha256=1575366c317d1d26310a621ae7488aa8780b79e60e4bc7167d8f5780108994c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
      "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pretty-midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592284 sha256=39ac0df89a2b98da6be31907f368c5285a826b933fc8324bbad99676a321ca15\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
      "Successfully built intervaltree pretty-midi\n",
      "Installing collected packages: pydub, protobuf, mido, jedi, intervaltree, pretty-midi, note_seq\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.2 which is incompatible.\n",
      "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed intervaltree-3.1.0 jedi-0.19.1 mido-1.3.2 note_seq-0.0.5 pretty-midi-0.2.10 protobuf-4.25.2 pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers evaluate accelerate\n",
    "!pip install wandb\n",
    "!pip install note_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KN2VoNGM7XUi",
    "outputId": "4595bd98-55c7-43b1-9c59-e3968ec95afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "fluidsynth is already the newest version (2.2.5-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Check if the code is running in Google Colab environment\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Inform the user about installing dependencies in Colab\n",
    "    print(\"Installing dependencies...\")\n",
    "\n",
    "    # Install fluidsynth and its development libraries using pip\n",
    "    !apt-get install fluidsynth\n",
    "    !apt-get install -qq libasound2-dev libjack-dev\n",
    "\n",
    "    # Install the pyfluidsynth library using pip\n",
    "    !pip install -qU pyfluidsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qjrncEFe7ZWL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "f_iFGg-U7btB",
    "outputId": "a2339325-8a79-46da-8a27-924462c8bc8f"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login into wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "99477b5614da46af8c85c2129aabbc1f",
      "b01e823243ad481789ec21daaaa8cfa4",
      "bd683b60144a4d3980ace220fc500fdb",
      "83daea79fd3d4dc99573562787030d3e",
      "4f57227fcb224cdf8bc10408ba03b520",
      "d955cc44cd2548e58e8d26376b6aad5f",
      "e4b4b10e7fc442eb98e7949d6a5c0140",
      "aa9216786541467eacd9760165c729c3",
      "460f9ee0f77646ae9ce1a31c2d530add",
      "2623c46d93324adcb8f8f560a7606ad0",
      "c787d4662b624a9a9c02339b367ee3c3",
      "1eb265b11318437284c976045dc75236",
      "ae44570360b04d3fa775ee9102e79456",
      "73b07a32b75846b690ff51bf7be4b5c0",
      "6c6212d6143f446d80d05dbf1ed74510",
      "fc1029f167d54d649190e2fb2cc8e5b1",
      "6d75955e194b48b09aa2ec60336a62a0",
      "66e977f652b34237900fc23648d0205f",
      "e1515aef92cb452aa307686a47b1cc04",
      "4c0c7795359d4d3da0eb79d678130036",
      "694e4fe66a2340b28c682cc674ba930e",
      "96a0241ea5da4729a730de2ea9eed429",
      "1b44f6381d3d4d549203494363f18ded",
      "37ce34889510478fac3e7957afe3f0d6",
      "eba7a76a36c74e58ade421de91e69c83",
      "6c5655a63c464929b739bac23707efea",
      "0dad4a2f53ca4c8a91b7281e298b494e",
      "b73b58a0eb7146669c45ebad33fd16a4",
      "22f6000adbe246bca152a105ecdfe971",
      "6f26eaacdddc40b4a0d16d38932c8e7b",
      "e5a808dc35e84c58bb1347af81452236",
      "3b0512bf9dd04ab9927d7150ae51703b"
     ]
    },
    "id": "b66O3eTo_Xl6",
    "outputId": "c41d4f42-663b-42ad-f2b3-8c457047fbe5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99477b5614da46af8c85c2129aabbc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYBi7VhS7d2y",
    "outputId": "96a3a211-4757-47eb-e2b6-c103a8d804cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main\n"
     ]
    }
   ],
   "source": [
    "%cd Thesis-main/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN1-by9X71_q",
    "outputId": "72635176-1deb-4219-c4f8-248d0bf907e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1SOLID_Data Exploration.ipynb'\t\t  params.py\n",
      "'2SOLID_Divide songs into Chunks.ipynb'   path\n",
      " 3SOLID_musicaiz_MMMTokenizer.ipynb\t 'Pickle file.ipynb'\n",
      " 4SOLID_Clean_data.ipynb\t\t  POP909\n",
      " 5SOLID_pretokenization_v3.ipynb\t  POP909_v2\n",
      " 5v2SOLID_pretokenization_v4.ipynb\t  POP909_v3\n",
      " 6SOLID_Model.ipynb\t\t\t  pop-pickle\n",
      " 6v2_SOLID_Model.ipynb\t\t\t  pop-pickle.zip\n",
      " 7SOLID_Hyperparameterisation.ipynb\t 'pretokenization v2.ipynb'\n",
      " 8SOLID_Tutorial_MMM_LMD_5_Sweeps.ipynb  'Read Me.txt'\n",
      " Archive\t\t\t\t  References.xlsx\n",
      "'CA2_SBA22222_Proposal v2.docx'\t\t  sweep.yaml\n",
      " create_dataset_mmm.py\t\t\t 'Thesis Write Up v2.docx'\n",
      " customtrainer.py\t\t\t 'Thesis Write Up v3.docx'\n",
      " logging.ipynb\t\t\t\t  tokenizer\n",
      " midi_data\t\t\t\t  tokenizer.json\n",
      " midi_data_v2\t\t\t\t  trained_tokenizer.json\n",
      " mmm_tokenizer_lmd_clean_SOLIDV2.ipynb\t  train.py\n",
      " output\t\t\t\t\t  utils.py\n",
      " output.xml\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNUoloqNCgol",
    "outputId": "d58b176f-37ea-405f-84af-95d93cbb7365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL='end'\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_LOG_MODEL='end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMur9GROJt0U",
    "outputId": "c72bc7ec-2262-44e5-e3ad-c0d673acf6b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'end'\n"
     ]
    }
   ],
   "source": [
    "!echo $WANDB_LOG_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38XiKM2NChLz",
    "outputId": "3b14e54e-b414-4d9c-ac13-d377e547b605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH='all'\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_WATCH='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spnFh669721a",
    "outputId": "fe1ac46f-f676-4a40-b54a-30fd373098a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 18:42:33.350416: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 18:42:33.350464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 18:42:33.351773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 18:42:34.574010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "usage: train.py [-h] [--output_dir OUTPUT_DIR] [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                [--evaluation_strategy EVALUATION_STRATEGY] [--save_strategy SAVE_STRATEGY]\n",
      "                [--eval_steps EVAL_STEPS] [--logging_steps LOGGING_STEPS]\n",
      "                [--logging_first_step LOGGING_FIRST_STEP] [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                [--save_steps SAVE_STEPS] [--lr_scheduler_type LR_SCHEDULER_TYPE]\n",
      "                [--learning_rate LEARNING_RATE] [--warmup_ratio WARMUP_RATIO]\n",
      "                [--weight_decay WEIGHT_DECAY] [--seed SEED]\n",
      "                [--load_best_model_at_end LOAD_BEST_MODEL_AT_END] [--report_to REPORT_TO]\n",
      "                [--prediction_loss_only PREDICTION_LOSS_ONLY] [--push_to_hub PUSH_TO_HUB]\n",
      "                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "\n",
      "Process hyper-parameters\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        Output directory\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        num train epochs\n",
      "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
      "                        per device train batch size\n",
      "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
      "                        per device eval batch size\n",
      "  --evaluation_strategy EVALUATION_STRATEGY\n",
      "                        evaluation strategy\n",
      "  --save_strategy SAVE_STRATEGY\n",
      "                        save strategy\n",
      "  --eval_steps EVAL_STEPS\n",
      "                        eval steps\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        logging steps\n",
      "  --logging_first_step LOGGING_FIRST_STEP\n",
      "                        logging first step\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT\n",
      "                        save total limit\n",
      "  --save_steps SAVE_STEPS\n",
      "                        save steps\n",
      "  --lr_scheduler_type LR_SCHEDULER_TYPE\n",
      "                        lr scheduler type\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        learning rate\n",
      "  --warmup_ratio WARMUP_RATIO\n",
      "                        warmup ratio\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        weight decay\n",
      "  --seed SEED           seed\n",
      "  --load_best_model_at_end LOAD_BEST_MODEL_AT_END\n",
      "                        load best model at end\n",
      "  --report_to REPORT_TO\n",
      "                        report to\n",
      "  --prediction_loss_only PREDICTION_LOSS_ONLY\n",
      "                        prediction loss only\n",
      "  --push_to_hub PUSH_TO_HUB\n",
      "                        push model to HF hub\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        gradient accumulation steps to compensate batch size\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kgv-VBor-vvp"
   },
   "source": [
    "# Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_BbB-3a768L",
    "outputId": "78cacad6-899e-4347-cac7-b0b60e216775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: sweep.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep with ID: \u001b[33mho9owhgg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909-pretokenization/sweeps/ho9owhgg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent musicgen/pop909-pretokenization/ho9owhgg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wandb sweep sweep.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNSVZ96r8Gry",
    "outputId": "f9621142-08d2-43bd-df88-76a725e3a135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wandb agent [OPTIONS] SWEEP_ID\n",
      "\n",
      "  Run the W&B agent\n",
      "\n",
      "Options:\n",
      "  -p, --project TEXT  The project of the sweep.\n",
      "  -e, --entity TEXT   The entity scope for the project.\n",
      "  --count INTEGER     The max number of runs for this agent.\n",
      "  --help              Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!wandb agent --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7HycP1zF1f7",
    "outputId": "e521b630-b52a-4fd0-a86b-f6ce025bc930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
      "2024-01-24 18:56:15,381 - wandb.wandb_agent - INFO - Running runs: []\n",
      "2024-01-24 18:56:15,768 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 18:56:15,768 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 2\n",
      "\tlearning_rate: 0.0009289080892233562\n",
      "2024-01-24 18:56:15,769 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=2 --learning_rate=0.0009289080892233562\n",
      "2024-01-24 18:56:19.502890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 18:56:19.502947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 18:56:19.504242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 18:56:20.740203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 18:56:20,776 - wandb.wandb_agent - INFO - Running runs: ['l0q2s1e1']\n",
      "Downloading readme: 100% 376/376 [00:00<00:00, 3.00MB/s]\n",
      "Downloading data: 100% 76.8M/76.8M [00:45<00:00, 1.68MB/s]\n",
      "Downloading data: 100% 70.7M/70.7M [00:30<00:00, 2.29MB/s]\n",
      "Downloading data: 100% 68.9M/68.9M [00:41<00:00, 1.65MB/s]\n",
      "Downloading data: 100% 67.2M/67.2M [00:28<00:00, 2.35MB/s]\n",
      "Downloading data: 100% 69.6M/69.6M [00:31<00:00, 2.22MB/s]\n",
      "Downloading data: 100% 67.3M/67.3M [00:28<00:00, 2.38MB/s]\n",
      "Downloading data: 100% 69.6M/69.6M [00:49<00:00, 1.42MB/s]\n",
      "Generating train split: 100% 177567/177567 [00:12<00:00, 13711.00 examples/s]\n",
      "tokenizer_config.json: 100% 146/146 [00:00<00:00, 872kB/s]\n",
      "tokenizer.json: 100% 602k/602k [00:00<00:00, 54.0MB/s]\n",
      "special_tokens_map.json: 100% 27.0/27.0 [00:00<00:00, 126kB/s]\n",
      "Map: 100% 159810/159810 [02:04<00:00, 1286.22 examples/s]\n",
      "Map: 100% 17757/17757 [00:13<00:00, 1318.17 examples/s]\n",
      "config.json: 100% 665/665 [00:00<00:00, 3.57MB/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190324-l0q2s1e1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msunny-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/l0q2s1e1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/9988 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 447772 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0% 0/9988 [00:01<?, ?it/s]\n",
      "2024-01-24 19:03:29,961 - wandb.wandb_agent - INFO - Cleaning up finished run: l0q2s1e1\n",
      "2024-01-24 19:03:30,351 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:03:30,351 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 1\n",
      "\tlearning_rate: 0.0027192618289645877\n",
      "2024-01-24 19:03:30,353 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=1 --learning_rate=0.0027192618289645877\n",
      "2024-01-24 19:03:34.080447: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:03:34.080497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:03:34.081785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:03:35.333216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:03:35,362 - wandb.wandb_agent - INFO - Running runs: ['xazidc26']\n",
      "Map: 100% 17757/17757 [00:14<00:00, 1229.18 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190359-xazidc26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/xazidc26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/19977 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 464756 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-sweep-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/xazidc26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190359-xazidc26/logs\u001b[0m\n",
      "2024-01-24 19:04:11,998 - wandb.wandb_agent - INFO - Cleaning up finished run: xazidc26\n",
      "2024-01-24 19:04:12,374 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:04:12,374 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 4\n",
      "\tlearning_rate: 0.0007403690296232116\n",
      "2024-01-24 19:04:12,375 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=4 --learning_rate=0.0007403690296232116\n",
      "2024-01-24 19:04:16.082132: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:04:16.082185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:04:16.083456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:04:17.333848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:04:17,385 - wandb.wandb_agent - INFO - Running runs: ['zlgasi8g']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190427-zlgasi8g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/zlgasi8g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/4994 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 466596 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcool-sweep-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/zlgasi8g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190427-zlgasi8g/logs\u001b[0m\n",
      "2024-01-24 19:04:38,477 - wandb.wandb_agent - INFO - Cleaning up finished run: zlgasi8g\n",
      "2024-01-24 19:04:38,869 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:04:38,869 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 4\n",
      "\tlearning_rate: 0.0015874696866744532\n",
      "2024-01-24 19:04:38,870 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=4 --learning_rate=0.0015874696866744532\n",
      "2024-01-24 19:04:42.580214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:04:42.580268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:04:42.581510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:04:43.841825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:04:43,877 - wandb.wandb_agent - INFO - Running runs: ['9j7m7e3h']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190454-9j7m7e3h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbright-sweep-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9j7m7e3h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/4994 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 467652 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbright-sweep-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9j7m7e3h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190454-9j7m7e3h/logs\u001b[0m\n",
      "2024-01-24 19:05:04,798 - wandb.wandb_agent - INFO - Cleaning up finished run: 9j7m7e3h\n",
      "2024-01-24 19:05:05,205 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2024-01-24 19:05:05,205 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tgradient_accumulation_steps: 2\n",
      "\tlearning_rate: 0.0024830196160634606\n",
      "2024-01-24 19:05:05,206 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --gradient_accumulation_steps=2 --learning_rate=0.0024830196160634606\n",
      "2024-01-24 19:05:08.885454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:05:08.885508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:05:08.886815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:05:10.164348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24 19:05:10,216 - wandb.wandb_agent - INFO - Running runs: ['9eiz4uu1']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/wandb/run-20240124_190519-9eiz4uu1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mkind-sweep-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/sweeps/6ce95s8n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9eiz4uu1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "  0% 0/9988 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 258, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
      "    outputs = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 390, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 331, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 212, in _attn\n",
      "    attn_weights = self.attn_dropout(attn_weights)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 573.06 MiB is free. Process 468693 has 14.19 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 761.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mkind-sweep-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/musicgen/pop909_protokenization/runs/9eiz4uu1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240124_190519-9eiz4uu1/logs\u001b[0m\n",
      "2024-01-24 19:05:31,164 - wandb.wandb_agent - ERROR - Detected 5 failed runs in a row, shutting down.\n",
      "2024-01-24 19:05:31,165 - wandb.wandb_agent - INFO - To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
     ]
    }
   ],
   "source": [
    "!wandb agent musicgen/pop909_protokenization/6ce95s8n --count 15\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-3cH3wp-zU1"
   },
   "source": [
    "# Final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvTv6TEaGAsj",
    "outputId": "d3b75a84-90e0-4d06-c3d1-b813de1228a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 19:15:05.107992: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 19:15:05.108047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 19:15:05.109302: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 19:15:06.340981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 116, in _service_connect\n",
      "    svc_iface._svc_connect(port=port)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/service/service_sock.py\", line 30, in _svc_connect\n",
      "    self._sock_client.connect(port=port)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 102, in connect\n",
      "    s.connect((\"localhost\", port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 289, in <module>\n",
      "    train(default_config)\n",
      "  File \"/content/Thesis-main/Thesis-main/Thesis-main/Thesis-main/train.py\", line 246, in train\n",
      "    run = wandb.init(project=params.wandb_project, job_type=\"training\", config=config)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1195, in init\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1172, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 189, in setup\n",
      "    self._wl = wandb_setup.setup(settings=setup_settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 327, in setup\n",
      "    ret = _setup(settings=settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 320, in _setup\n",
      "    wl = _WandbSetup(settings=settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 303, in __init__\n",
      "    _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 114, in __init__\n",
      "    self._setup()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 250, in _setup\n",
      "    self._setup_manager()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 277, in _setup_manager\n",
      "    self._manager = wandb_manager._Manager(settings=self._settings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 153, in __init__\n",
      "    wandb._sentry.reraise(e)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 154, in reraise\n",
      "    raise exc.with_traceback(sys.exc_info()[2])\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 151, in __init__\n",
      "    self._service_connect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 125, in _service_connect\n",
      "    raise ManagerConnectionRefusedError(message)\n",
      "wandb.sdk.wandb_manager.ManagerConnectionRefusedError: Connection to wandb service failed: [Errno 111] Connection refused. \n"
     ]
    }
   ],
   "source": [
    "!python train.py --learning_rate=0.0005 --per_device_train_batch_size=8 --per_device_eval_batch_size=4 --num_train_epochs=10 --push_to_hub=True --eval_steps=4994 --logging_steps=4994 --save_steps=4994 --output_dir=\"pop909-epochs10\" --gradient_accumulation_steps=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bV8g0GkCAm6I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0dad4a2f53ca4c8a91b7281e298b494e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b44f6381d3d4d549203494363f18ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22f6000adbe246bca152a105ecdfe971",
      "placeholder": "​",
      "style": "IPY_MODEL_6f26eaacdddc40b4a0d16d38932c8e7b",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "1eb265b11318437284c976045dc75236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22f6000adbe246bca152a105ecdfe971": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2623c46d93324adcb8f8f560a7606ad0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37ce34889510478fac3e7957afe3f0d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5a808dc35e84c58bb1347af81452236",
      "placeholder": "​",
      "style": "IPY_MODEL_3b0512bf9dd04ab9927d7150ae51703b",
      "value": "Login successful"
     }
    },
    "3b0512bf9dd04ab9927d7150ae51703b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "460f9ee0f77646ae9ce1a31c2d530add": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c0c7795359d4d3da0eb79d678130036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f57227fcb224cdf8bc10408ba03b520": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_73b07a32b75846b690ff51bf7be4b5c0",
      "style": "IPY_MODEL_6c6212d6143f446d80d05dbf1ed74510",
      "tooltip": ""
     }
    },
    "66e977f652b34237900fc23648d0205f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1515aef92cb452aa307686a47b1cc04",
      "placeholder": "​",
      "style": "IPY_MODEL_4c0c7795359d4d3da0eb79d678130036",
      "value": "Connecting..."
     }
    },
    "694e4fe66a2340b28c682cc674ba930e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eba7a76a36c74e58ade421de91e69c83",
      "placeholder": "​",
      "style": "IPY_MODEL_6c5655a63c464929b739bac23707efea",
      "value": "Token is valid (permission: write)."
     }
    },
    "6c5655a63c464929b739bac23707efea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c6212d6143f446d80d05dbf1ed74510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "6d75955e194b48b09aa2ec60336a62a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f26eaacdddc40b4a0d16d38932c8e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73b07a32b75846b690ff51bf7be4b5c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83daea79fd3d4dc99573562787030d3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_1eb265b11318437284c976045dc75236",
      "style": "IPY_MODEL_ae44570360b04d3fa775ee9102e79456",
      "value": true
     }
    },
    "96a0241ea5da4729a730de2ea9eed429": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dad4a2f53ca4c8a91b7281e298b494e",
      "placeholder": "​",
      "style": "IPY_MODEL_b73b58a0eb7146669c45ebad33fd16a4",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "99477b5614da46af8c85c2129aabbc1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_694e4fe66a2340b28c682cc674ba930e",
       "IPY_MODEL_96a0241ea5da4729a730de2ea9eed429",
       "IPY_MODEL_1b44f6381d3d4d549203494363f18ded",
       "IPY_MODEL_37ce34889510478fac3e7957afe3f0d6"
      ],
      "layout": "IPY_MODEL_e4b4b10e7fc442eb98e7949d6a5c0140"
     }
    },
    "aa9216786541467eacd9760165c729c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae44570360b04d3fa775ee9102e79456": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b01e823243ad481789ec21daaaa8cfa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa9216786541467eacd9760165c729c3",
      "placeholder": "​",
      "style": "IPY_MODEL_460f9ee0f77646ae9ce1a31c2d530add",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "b73b58a0eb7146669c45ebad33fd16a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd683b60144a4d3980ace220fc500fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_2623c46d93324adcb8f8f560a7606ad0",
      "placeholder": "​",
      "style": "IPY_MODEL_c787d4662b624a9a9c02339b367ee3c3",
      "value": ""
     }
    },
    "c787d4662b624a9a9c02339b367ee3c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d955cc44cd2548e58e8d26376b6aad5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc1029f167d54d649190e2fb2cc8e5b1",
      "placeholder": "​",
      "style": "IPY_MODEL_6d75955e194b48b09aa2ec60336a62a0",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "e1515aef92cb452aa307686a47b1cc04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4b4b10e7fc442eb98e7949d6a5c0140": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "e5a808dc35e84c58bb1347af81452236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eba7a76a36c74e58ade421de91e69c83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc1029f167d54d649190e2fb2cc8e5b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
