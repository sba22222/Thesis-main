{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1. Why is the context length set to 2048? Look up hugging face documentation\n",
    "2. What is the number you put in for 'xquery = 20100000' what is it related to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GPT-2 Language Model on the POP909 Dataset\n",
    "\n",
    "The below script is dedicated to training a language model on the POP909 dataset using the GPT-2 architecture. It begins by installing necessary libraries and importing required modules. Then it loads the preprocessed dataset and tokenizes it using the Hugging Face tokenizer.\n",
    "\n",
    "The script proceeds to define the GPT-2 model architecture and configure it based on specified parameters such as the number of layers, attention heads, and embedding dimension. It calculates and prints the size of the GPT-2 model in terms of parameters.\n",
    "\n",
    "Next, it prepares the data for training by creating a data collator that handles batch preparation and creates language model labels. It applies the data collator to a small subset of the training data to verify its functionality.\n",
    "\n",
    "The script then sets up the training environment, including logging configurations for Weights & Biases (WandB) integration and Hugging Face Hub login. It defines a custom trainer class that extends the Trainer class provided by the transformers library. This custom trainer includes additional functionality to log prediction distributions during evaluation.\n",
    "\n",
    "Training hyperparameters and configuration parameters are defined, and a WandB run is initiated to monitor training progress and log metrics.\n",
    "\n",
    "The model training loop is executed using the Trainer object, which handles training epochs, batch processing, and evaluation. During evaluation, the custom trainer logs generated audio samples for qualitative analysis.\n",
    "\n",
    "Once training is complete, the WandB run is finished, and the model checkpoint is saved to the specified output directory. Overall, this notebook provides a comprehensive pipeline for training a GPT-2 language model on the POP909 dataset, including data preprocessing, model setup, training, and result logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "print(locale.getpreferredencoding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yPGNyAscsvzD"
   },
   "outputs": [],
   "source": [
    "## Install necessary libraries\n",
    "# !pip install datasets\n",
    "# !pip install wandb\n",
    "# !pip install note_seq\n",
    "# !pip install transformers[torch]\n",
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the code is running in Google Colab environment\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Inform the user about installing dependencies in Colab\n",
    "    print(\"Installing dependencies...\")\n",
    "\n",
    "    # Install fluidsynth and its development libraries using pip\n",
    "    !apt-get install fluidsynth\n",
    "    !apt-get install -qq libasound2-dev libjack-dev\n",
    "\n",
    "    # Install the pyfluidsynth library using pip\n",
    "    !pip install -qU pyfluidsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naomi\\anaconda3\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import note_seq\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoConfig, GPT2LMHeadModel, DataCollatorForLanguageModeling, set_seed, Trainer, TrainingArguments\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "COgCtDY2bJpQ"
   },
   "outputs": [],
   "source": [
    "# Set the Protocol Buffers Python implementation to \"python\"\n",
    "# This line is used to resolve compatibility issues related to Protocol Buffers (protobuf)\n",
    "# It explicitly selects the pure Python implementation of the protobuf library\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for WandB (Weights & Biases) integration\n",
    "wandb_project = \"pop909_musicgen\"\n",
    "entity = \"musicgen\"\n",
    "data_processed = \"pop909_processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4vQF0ZdtPzP"
   },
   "source": [
    "## Download Dataset and tokenizer from Hugging Face\n",
    "\n",
    "In the pretokenization notebook, we trained a tokenizer. We'll use it here first to do some basic EDA to understand our data and what type of model size is better (number of layers, heads, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3Se37G7o30ap"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/naomi/.cache/huggingface/datasets/aimusicgen___parquet/aimusicgen--pop909_clean_data-8139a41134aae9f9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 29930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3326\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a dataset named \"aimusicgen/pop909_clean_data\" using the Hugging Face datasets library\n",
    "# The split parameter is set to \"train\" to load the training split of the dataset\n",
    "ds = load_dataset(\"aimusicgen/pop909_clean_data\", split=\"train\")\n",
    "\n",
    "# Split the loaded dataset into training and testing sets using train_test_split method\n",
    "# The test_size parameter specifies the fraction of the dataset to include in the test split (here, 10%)\n",
    "# The shuffle parameter is set to True to shuffle the data before splitting\n",
    "raw_datasets = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "# Instantiate a tokenizer using the AutoTokenizer class from the transformers library\n",
    "# The \"tokenizer\" argument is the pretrained tokenizer from the previous notebook\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aimusicgen/pop909_tokenizer\")\n",
    "\n",
    "# Display the raw datasets, which now include both training and testing splits\n",
    "# The raw_datasets variable now contains a tuple with two elements: training and testing datasets\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the data suffled before it is split into training and testing?\n",
    "\n",
    "The data is suffled to help introduce **randomness** into the data. This is important because it prevents any inherent order in the dataset from influencing the learning algorithm. If the data is ordered in a certain way (e.g., all samples of one class followed by another), shuffling helps ensure that the model sees a representative mix of samples from all classes during both training and testing.\n",
    "\n",
    "Some algorithms might perform differently or learn **biased** patterns if trained on data with a specific order. By shuffling the data, you reduce the risk of the model learning patterns based on the order of the examples.\n",
    "\n",
    "Shuffling contributes to better **generalization** of the model. If the model is exposed to diverse examples during training (rather than learning specific patterns related to the order of the data), it is more likely to perform well on new, unseen data.\n",
    "\n",
    "When splitting a dataset into training and testing sets, shuffling ensures that both sets contain a representative mix of examples. This is important, especially in scenarios like **cross-validation**, where you repeatedly split the data into different training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cCxK_DEBREOs"
   },
   "outputs": [],
   "source": [
    "# Define the context length for tokenization\n",
    "# In this case, it is set to 512, meaning the input sequences will be truncated or padded to this length\n",
    "context_length = 512\n",
    "\n",
    "# Define a tokenization function named \"tokenize\" that takes an element as input\n",
    "def tokenize(element):\n",
    "    # Use the tokenizer to process the \"text\" field of the input element\n",
    "    # Set truncation to True to truncate sequences longer than the specified context length\n",
    "    # Set max_length to the context_length to ensure all sequences have the same length\n",
    "    # Set padding to False to avoid adding padding tokens to the sequences\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    # Return a dictionary containing the \"input_ids\" field from the tokenizer outputs\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example from the training split of the raw dataset (index 1000)\n",
    "selected_example = raw_datasets[\"train\"][1000]\n",
    "\n",
    "# Tokenize the selected example using the tokenize function defined earlier\n",
    "# The tokenize function processes the \"text\" field of the input example\n",
    "# and returns a dictionary containing the \"input_ids\" field with tokenized representation\n",
    "tk_sample = tokenize(selected_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of tk_sample ids 413\n",
      "tk_sample {'input_ids': [74, 24, 22, 9, 5, 47, 10, 53, 61, 98, 11, 52, 60, 82, 46, 11, 49, 10, 19, 14, 18, 7, 30, 11, 29, 47, 6, 46, 11, 53, 10, 61, 52, 11, 60, 6, 59, 36, 47, 88, 7, 48, 81, 97, 13, 58, 6, 35, 46, 6, 59, 36, 47, 41, 58, 35, 7, 46, 87, 8, 9, 5, 59, 55, 38, 65, 41, 58, 54, 37, 7, 38, 55, 59, 12, 64, 6, 37, 14, 54, 14, 58, 14, 30, 47, 49, 53, 98, 5, 29, 46, 48, 52, 97, 8, 9, 5, 71, 65, 10, 78, 32, 70, 10, 71, 38, 6, 77, 11, 55, 55, 70, 11, 54, 54, 59, 59, 13, 58, 61, 6, 37, 7, 31, 64, 15, 60, 53, 65, 53, 47, 58, 5, 64, 52, 11, 46, 52, 8, 9, 5, 47, 47, 36, 7, 46, 7, 35, 46, 61, 53, 69, 11, 52, 60, 71, 19, 10, 70, 61, 36, 10, 60, 47, 61, 10, 46, 53, 12, 35, 7, 18, 52, 59, 6, 68, 12, 60, 58, 55, 59, 36, 94, 11, 58, 6, 69, 10, 40, 11, 35, 19, 13, 32, 6, 18, 7, 93, 68, 6, 31, 6, 39, 14, 54, 8, 23, 24, 22, 9, 5, 47, 10, 53, 61, 98, 11, 52, 60, 82, 46, 11, 49, 10, 19, 14, 18, 7, 30, 11, 29, 47, 6, 46, 11, 53, 10, 61, 52, 11, 60, 6, 59, 36, 47, 88, 7, 48, 81, 97, 13, 58, 6, 35, 46, 6, 59, 36, 47, 41, 58, 35, 7, 46, 87, 8, 9, 5, 59, 55, 38, 65, 41, 58, 54, 37, 7, 38, 55, 59, 12, 64, 6, 37, 14, 54, 14, 58, 14, 30, 47, 49, 53, 98, 5, 29, 46, 48, 52, 97, 8, 9, 5, 71, 65, 10, 78, 32, 70, 10, 71, 38, 6, 77, 11, 55, 55, 70, 11, 54, 54, 59, 59, 13, 58, 61, 6, 37, 7, 31, 64, 15, 60, 53, 65, 53, 47, 58, 5, 64, 52, 11, 46, 52, 8, 9, 5, 47, 47, 36, 7, 46, 7, 35, 46, 61, 53, 69, 11, 52, 60, 71, 19, 10, 70, 61, 36, 10, 60, 47, 61, 10, 46, 53, 12, 35, 7, 18, 52, 59, 6, 68, 12, 60, 58, 55, 59, 36, 94, 11, 58, 6, 69, 10, 40, 11, 35, 19, 13, 32, 6, 18, 7, 93, 68, 6, 31, 6, 39, 14, 54, 8, 23]}\n"
     ]
    }
   ],
   "source": [
    "# Print the length of the 'input_ids' field in the tokenized sample\n",
    "# This indicates the number of tokens in the tokenized representation of the input text\n",
    "print(f\"Len of tk_sample ids {len(tk_sample['input_ids'])}\")\n",
    "\n",
    "# Print the entire tokenized sample\n",
    "# The tokenized sample is a dictionary containing the 'input_ids' field\n",
    "print(f\"tk_sample {tk_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see only 742 tk_sample ids came back when there was an index of 1,000. This is possibly due to padding. It was set to *padding=False* in the *tokenize* function. Without padding, the resulting *input_ids* will not be padded to the maximum length, and if the original text is shorter than the specified *max_length*, the tokenized sequence will be shorter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4zunRo-CfEHr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 29930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3326\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the tokenize function to the entire raw dataset using the map method\n",
    "# The tokenize function is applied in a batched manner, improving efficiency\n",
    "# The remove_columns parameter is set to remove the columns from the raw dataset \n",
    "# (excluding the \"train\" split) after tokenization, as they are no longer needed\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize,                # The tokenization function to be applied\n",
    "    batched=True,            # Tokenize in batches for efficiency\n",
    "    remove_columns=raw_datasets[\"train\"].column_names  # Remove unnecessary columns after tokenization\n",
    ")\n",
    "\n",
    "# Display the resulting tokenized datasets\n",
    "# The tokenized_datasets variable now contains both training and testing splits with tokenized representations\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CICtm9YRkbgQ"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "S-3Bmvu1ihqA"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for model architecture\n",
    "# These parameters can be adjusted based on the size of the data and the specific requirements of the task\n",
    "\n",
    "n_layer = 6 # Number of layers in the transformer model\n",
    "n_head = 8 # Number of attention heads in each transformer layer\n",
    "n_emb = 512 # Dimensionality of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "OA1QEtv2lIX2"
   },
   "outputs": [],
   "source": [
    "# Use Hugging Face's AutoConfig to create a configuration for the GPT-2 model\n",
    "# The configuration is based on the \"gpt2\" pre-trained model, and some parameters are customized\n",
    "\n",
    "# Define the configuration using AutoConfig\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",                               # Base model: \"gpt2\"\n",
    "    vocab_size=len(tokenizer),            # Vocabulary size based on the tokenizer\n",
    "    n_positions=context_length,           # Maximum position embeddings (context length)\n",
    "    n_layer=n_layer,                      # Number of transformer layers\n",
    "    n_head=n_head,                        # Number of attention heads in each layer\n",
    "    pad_token_id=tokenizer.pad_token_id,  # ID of the padding token\n",
    "    bos_token_id=tokenizer.bos_token_id,  # ID of the beginning-of-sequence token\n",
    "    eos_token_id=tokenizer.eos_token_id,  # ID of the end-of-sequence token\n",
    "    n_embd=n_emb                           # Dimensionality of the embedding layer\n",
    ")\n",
    "\n",
    "# Set the num_attention_heads after creating the configuration\n",
    "    #config.num_attention_heads = 0\n",
    "\n",
    "# Create an instance of the GPT-2 language model using the configured parameters\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zfi_c7Ijmjav"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 20.1M parameters\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the size of the GPT-2 model in terms of parameters\n",
    "# The size is measured in millions (M) of parameters\n",
    "\n",
    "# Calculate the total number of parameters in the GPT-2 model\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "\n",
    "# Print the size of the GPT-2 model in millions of parameters\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBfWG2iZBLop"
   },
   "source": [
    "Create a datacollator to will take care of preparing the data:\n",
    "\n",
    "\"Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the DataCollatorForLanguageModeling collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don’t need to duplicate the input_ids.\n",
    "\n",
    "Note that DataCollatorForLanguageModeling supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument mlm=False:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "xGg8htNBnF_m"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "KToTnxF8Ckkn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 359])\n",
      "attention_mask shape: torch.Size([5, 359])\n",
      "labels shape: torch.Size([5, 359])\n",
      "Collated outputs: {'input_ids': tensor([[ 74,  24,  22,  ...,   3,   3,   3],\n",
      "        [ 74,  24,  22,  ...,   3,   3,   3],\n",
      "        [ 74,  24,  22,  ..., 111,   8,  23],\n",
      "        [ 74,  24,  22,  ...,   3,   3,   3],\n",
      "        [ 74,  24,  22,  ...,   3,   3,   3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  74,   24,   22,  ..., -100, -100, -100],\n",
      "        [  74,   24,   22,  ..., -100, -100, -100],\n",
      "        [  74,   24,   22,  ...,  111,    8,   23],\n",
      "        [  74,   24,   22,  ..., -100, -100, -100],\n",
      "        [  74,   24,   22,  ..., -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "# Test the data collator on a small subset of the training data\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "\n",
    "# Print the shapes of the collated outputs\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "# Display the collated outputs\n",
    "print(f\"Collated outputs: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "OLNX31-WDMcX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaomitunstead\u001b[0m (\u001b[33mmusicgen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login into wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "8MpkHZ6qO_Zt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL='checkpoint'\n"
     ]
    }
   ],
   "source": [
    "# Set the WandB environment variable to log the model checkpoint\n",
    "%env WANDB_LOG_MODEL='checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lyOSwB1dJHcW"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584c0a77358a406e9bec25be2f95cc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login into Hugging Face\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Ry-BYiGMJiq2"
   },
   "outputs": [],
   "source": [
    "# Create the args for out trainer\n",
    "\n",
    "# Get the output directory with timestamp.\n",
    "output_path = \"output\"\n",
    "steps = 5000\n",
    "\n",
    "# Define training configuration parameters\n",
    "# Commented parameters correspond to the small model\n",
    "config = {\"output_dir\": output_path,\n",
    "          \"num_train_epochs\": 1,\n",
    "          \"per_device_train_batch_size\": 8,\n",
    "          \"per_device_eval_batch_size\": 4,\n",
    "          \"evaluation_strategy\": \"steps\",\n",
    "          \"save_strategy\": \"steps\",\n",
    "          \"eval_steps\": steps,\n",
    "          \"logging_steps\":steps,\n",
    "          \"logging_first_step\": True,\n",
    "          \"save_total_limit\": 5,\n",
    "          \"save_steps\": steps,\n",
    "          \"lr_scheduler_type\": \"cosine\",\n",
    "          \"learning_rate\":5e-4,\n",
    "          \"warmup_ratio\": 0.01,\n",
    "          \"weight_decay\": 0.01,\n",
    "          \"seed\": 1,\n",
    "          \"load_best_model_at_end\": True,\n",
    "          \"report_to\": \"wandb\"}\n",
    "\n",
    "# Create Namespace object with configuration\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "M07MnTwqJ34f"
   },
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Zby2k2tQNYFG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\naomi\\Thesis\\Thesis\\Thesis-main\\wandb\\run-20240120_213352-z5mroc4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/musicgen/pop909-pretokenization/runs/z5mroc4i' target=\"_blank\">vocal-water-1</a></strong> to <a href='https://wandb.ai/musicgen/pop909-pretokenization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/musicgen/pop909-pretokenization' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/musicgen/pop909-pretokenization/runs/z5mroc4i' target=\"_blank\">https://wandb.ai/musicgen/pop909-pretokenization/runs/z5mroc4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and start a new WandB run for training\n",
    "run = wandb.init(project=wandb_project, job_type=\"training\", config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Y-OMMfag8GEo"
   },
   "outputs": [],
   "source": [
    "# Code for converting token sequences to NoteSequences with audio-related information\n",
    "\n",
    "# Constants for note durations\n",
    "NOTE_LENGTH_16TH_120BPM = 0.25 * 60 / 120\n",
    "BAR_LENGTH_120BPM = 4.0 * 60 / 120\n",
    "\n",
    "def token_sequence_to_note_sequence(token_sequence, use_program=True, use_drums=True, instrument_mapper=None, only_piano=False):\n",
    "    \"\"\"\n",
    "    Convert a token sequence to a NoteSequence with audio-related information.\n",
    "    Args:\n",
    "        token_sequence (list or str): Token sequence representing musical information.\n",
    "        use_program (bool): Whether to use program information for instruments.\n",
    "        use_drums (bool): Whether to include drums in the output.\n",
    "        instrument_mapper (dict): Mapping of instrument names to MIDI program numbers.\n",
    "        only_piano (bool): Whether to include only piano instruments in the output.\n",
    "    Returns:\n",
    "        note_sequence (NoteSequence): Converted NoteSequence.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(token_sequence, str):\n",
    "        token_sequence = token_sequence.split()\n",
    "\n",
    "    note_sequence = empty_note_sequence()\n",
    "\n",
    "    # Render all notes.\n",
    "    current_program = 1\n",
    "    current_is_drum = False\n",
    "    current_instrument = 0\n",
    "    track_count = 0\n",
    "    for token_index, token in enumerate(token_sequence):\n",
    "\n",
    "        if token == \"PIECE_START\":\n",
    "            pass\n",
    "        elif token == \"PIECE_END\":\n",
    "            print(\"The end.\")\n",
    "            break\n",
    "        elif token == \"TRACK_START\":\n",
    "            current_bar_index = 0\n",
    "            track_count += 1\n",
    "            pass\n",
    "        elif token == \"TRACK_END\":\n",
    "            pass\n",
    "        elif token == \"KEYS_START\":\n",
    "            pass\n",
    "        elif token == \"KEYS_END\":\n",
    "            pass\n",
    "        elif token.startswith(\"KEY=\"):\n",
    "            pass\n",
    "        elif token.startswith(\"INST\"):\n",
    "            instrument = token.split(\"=\")[-1]\n",
    "            if instrument != \"DRUMS\" and use_program:\n",
    "                if instrument_mapper is not None:\n",
    "                    if instrument in instrument_mapper:\n",
    "                        instrument = instrument_mapper[instrument]\n",
    "                current_program = int(instrument)\n",
    "                current_instrument = track_count\n",
    "                current_is_drum = False\n",
    "            if instrument == \"DRUMS\" and use_drums:\n",
    "                current_instrument = 0\n",
    "                current_program = 0\n",
    "                current_is_drum = True\n",
    "        elif token == \"BAR_START\":\n",
    "            current_time = current_bar_index * BAR_LENGTH_120BPM\n",
    "            current_notes = {}\n",
    "        elif token == \"BAR_END\":\n",
    "            current_bar_index += 1\n",
    "            pass\n",
    "        elif token.startswith(\"NOTE_ON\"):\n",
    "            pitch = int(token.split(\"=\")[-1])\n",
    "            note = note_sequence.notes.add()\n",
    "            note.start_time = current_time\n",
    "            note.end_time = current_time + 4 * NOTE_LENGTH_16TH_120BPM\n",
    "            note.pitch = pitch\n",
    "            note.instrument = current_instrument\n",
    "            note.program = current_program\n",
    "            note.velocity = 80\n",
    "            note.is_drum = current_is_drum\n",
    "            current_notes[pitch] = note\n",
    "        elif token.startswith(\"NOTE_OFF\"):\n",
    "            pitch = int(token.split(\"=\")[-1])\n",
    "            if pitch in current_notes:\n",
    "                note = current_notes[pitch]\n",
    "                note.end_time = current_time\n",
    "        elif token.startswith(\"TIME_DELTA\"):\n",
    "            delta = float(token.split(\"=\")[-1]) * NOTE_LENGTH_16TH_120BPM\n",
    "            current_time += delta\n",
    "        elif token.startswith(\"DENSITY=\"):\n",
    "            pass\n",
    "        elif token == \"[PAD]\":\n",
    "            pass\n",
    "        else:\n",
    "            #print(f\"Ignored token {token}.\")\n",
    "            pass\n",
    "\n",
    "    # Make the instruments right.\n",
    "    instruments_drums = []\n",
    "    for note in note_sequence.notes:\n",
    "        pair = [note.program, note.is_drum]\n",
    "        if pair not in instruments_drums:\n",
    "            instruments_drums += [pair]\n",
    "        note.instrument = instruments_drums.index(pair)\n",
    "\n",
    "    if only_piano:\n",
    "        for note in note_sequence.notes:\n",
    "            if not note.is_drum:\n",
    "                note.instrument = 0\n",
    "                note.program = 0\n",
    "\n",
    "    return note_sequence\n",
    "\n",
    "def empty_note_sequence(qpm=120.0, total_time=0.0):\n",
    "    \"\"\"\n",
    "    Create an empty NoteSequence with specified tempo and total time.\n",
    "    Args:\n",
    "        qpm (float): Quarter notes per minute (tempo).\n",
    "        total_time (float): Total time of the NoteSequence.\n",
    "    Returns:\n",
    "        note_sequence (NoteSequence): Empty NoteSequence.\n",
    "    \"\"\"\n",
    "    note_sequence = note_seq.protobuf.music_pb2.NoteSequence()\n",
    "    note_sequence.tempos.add().qpm = qpm\n",
    "    note_sequence.ticks_per_quarter = note_seq.constants.STANDARD_PPQ\n",
    "    note_sequence.total_time = total_time\n",
    "    return note_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "CYdVIUZ_7xWR"
   },
   "outputs": [],
   "source": [
    "# first create a custom trainer to log prediction distribution\n",
    "# Set the sample rate for audio processing\n",
    "SAMPLE_RATE=44100\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader,\n",
    "        description,\n",
    "        prediction_loss_only=None,\n",
    "        ignore_keys=None,\n",
    "        metric_key_prefix=\"eval\",\n",
    "    ):\n",
    "        # Call super class method to get the eval outputs\n",
    "        eval_output = super().evaluation_loop(\n",
    "            dataloader,\n",
    "            description,\n",
    "            prediction_loss_only,\n",
    "            ignore_keys,\n",
    "            metric_key_prefix,\n",
    "        )\n",
    "\n",
    "         # Log the prediction distribution using `wandb.Histogram` method.\n",
    "        if wandb.run is not None:\n",
    "            # Encode a starting token to begin the generation\n",
    "            input_ids = self.tokenizer.encode(\"PIECE_START\", return_tensors=\"pt\").cuda()\n",
    "\n",
    "            # Generate more tokens for each voice\n",
    "            for voice_num in range(1, 5):\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=2048,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.75, # Set temperature for sampling (higher values for more randomness, lower for more determinism)\n",
    "                    #top_p = 0.9, # Set top-p sampling parameters (nucleus sampling) to control diversity\n",
    "                    #top_k = 50, # Set top-k sampling parameters to restrict generation to the top-k most likely tokens\n",
    "                    eos_token_id=self.tokenizer.encode(\"TRACK_END\")[0]\n",
    "                )\n",
    "\n",
    "                # Decode the generated tokens into a token sequence\n",
    "                token_sequence = self.tokenizer.decode(generated_ids[0])\n",
    "\n",
    "                # Convert the token sequence into a NoteSequence\n",
    "                note_sequence = token_sequence_to_note_sequence(token_sequence)\n",
    "\n",
    "                # Synthesize the audio from the NoteSequence\n",
    "                synth = note_seq.fluidsynth\n",
    "                array_of_floats = synth(note_sequence, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "                # Convert the float audio samples to int16 format\n",
    "                int16_data = note_seq.audio_io.float_samples_to_int16(array_of_floats)\n",
    "\n",
    "                # Log the generated audio using the wandb.Audio method\n",
    "                wandb.log({\"Generated_audio_voice_\" + str(voice_num): wandb.Audio(int16_data, SAMPLE_RATE)})\n",
    "\n",
    "        # Return the evaluation output\n",
    "        return eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "JVsTKx3t-hLP"
   },
   "outputs": [],
   "source": [
    "# Create TrainingArguments object with training configuration \n",
    "train_args = TrainingArguments(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "D9hBshTJ9coM"
   },
   "outputs": [],
   "source": [
    " # Create training arguments\n",
    "train_args = TrainingArguments(**config)\n",
    "\n",
    "# Initialize the custom trainer for training the model\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    # compute_metrics=compute_metrics_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SEgCmFiz9v6S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='3742' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  22/3742 09:32 < 29:35:03, 0.03 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1843\u001b[0m ):\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2685\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2706\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2707\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2708\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2709\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1077\u001b[0m     input_ids,\n\u001b[0;32m   1078\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1079\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1080\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1081\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1082\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1083\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1084\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1085\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1086\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1087\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1088\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1089\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    892\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    898\u001b[0m     )\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[0;32m    901\u001b[0m         hidden_states,\n\u001b[0;32m    902\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m    903\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    904\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[0;32m    905\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    906\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    907\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    908\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    909\u001b[0m     )\n\u001b[0;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m--> 354\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m    355\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m    356\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:105\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    104\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[1;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39maddmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iomIVDkb8PeZ"
   },
   "outputs": [],
   "source": [
    "# call wandb.finish() to finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPkkleKq-zzL"
   },
   "source": [
    "Model Performance Evaluation: Validation and training loss provide insights into how well your model is performing during training and validation phases. High training loss might indicate that the model is not learning properly, while high validation loss might indicate overfitting or poor generalization.\n",
    "\n",
    "Overfitting Detection: Monitoring the difference between training and validation loss helps you detect overfitting. If your model performs well on the training data but poorly on the validation data, it's a sign that the model is overfitting to the training data and not generalizing well.\n",
    "\n",
    "Hyperparameter Tuning: Understanding the loss metrics helps in tuning hyperparameters like learning rate, batch size, and regularization techniques. Adjusting these parameters based on the loss metrics can improve model performance.\n",
    "\n",
    "Model Selection: Comparing validation loss across different models helps in selecting the best-performing model for deployment. Lower validation loss generally indicates a better-performing model.\n",
    "\n",
    "Early Stopping: Monitoring the validation loss can help implement early stopping strategies. When validation loss stops improving or starts to increase, it's an indication to stop training, preventing the model from overfitting further.\n",
    "\n",
    "Feedback Loop: Validation loss provides feedback on the effectiveness of changes made to the model architecture, data preprocessing, or other components. This iterative process helps in refining the model and improving its performance.\n",
    "\n",
    "Communication: Reporting validation and training loss is standard practice in research and applied deep learning projects. It enables clear communication of the model's performance and its behavior during training.\n",
    "\n",
    "Overall, understanding and monitoring validation and training loss are essential for ensuring the effectiveness, generalization, and performance of your deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing Loss: During training, the loss should generally decrease over time. This indicates that the model is learning and adjusting its parameters to better fit the training data.\n",
    "\n",
    "Convergence: Ideally, both training and validation losses should converge to a stable value. This suggests that the model has learned as much as it can from the available data.\n",
    "\n",
    "Training Loss: It's common for the training loss to be lower than the validation loss. This is because the model is optimized specifically for the training data. However, if the training loss is significantly lower than the validation loss, it could indicate overfitting.\n",
    "\n",
    "Validation Loss: The validation loss should be relatively low, indicating that the model generalizes well to unseen data. If the validation loss is substantially higher than the training loss, it suggests overfitting.\n",
    "\n",
    "Baseline Comparison: Compare your model's performance with a baseline model or previous results in the literature if available. This gives you a reference point for evaluating the adequacy of your model's performance.\n",
    "\n",
    "Domain Knowledge: Consider the context of your problem and what level of performance is practically useful. In some cases, even relatively high losses may be acceptable if the model achieves the desired outcome.\n",
    "\n",
    "Regularization: Regularization techniques like dropout, weight decay, or early stopping can help control overfitting and improve generalization, leading to lower validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
